# ----------------------------------------------------------------------
# LALSuite: Gitlab-CI configuration
#
# This file is complicated, but hopefully robust. If you have any
# questions, please email lal-discuss@ligo.org, or open a question
# ticket at https://git.ligo.org/lscsoft/lalsuite/-/issues/
# ----------------------------------------------------------------------

# -- setup --------------------------------------

stages:
  - tarballs
  # build packages for each subpackage
  - LAL
  - LALFrame
  - LALMetaIO
  - LALSimulation
  - LALBurst
  - LALInspiral
  - LALInference
  - LALPulsar
  - LALApps
  - wheels
  # end-to-end tests
  - integration tests
  - compiler tests
  - platform tests
  - upgrade tests
  # build containers
  - docker
  # quality checks
  - coverage
  - lint
  # documentation
  - documentation
  # deploy packages
  - deploy

include:
  # include job templates from the gitlab-ci-templates repo
  # see https://computing.docs.ligo.org/gitlab-ci-templates/
  - project: computing/gitlab-ci-templates
    file:
      # helpers for codequality jobs
      - codequality.yml
      # helpers for debian builds
      - debian.yml
      # helpers for RHEL builds
      - rhel.yml
      # a flake8 job template
      - python.yml

  # CI rules
  - local: '/.gitlab/ci/rules.yml'

  # build packages for each subpackage
  - local: '/.gitlab/ci/pkg.yml'

  # end-to-end tests
  - local: '/.gitlab/ci/integration.yml'
  - local: '/.gitlab/ci/compilers.yml'
  - local: '/.gitlab/ci/platform.yml'
  - local: '/.gitlab/ci/upgrade.yml'

  # build containers
  - local: '/.gitlab/ci/containers.yml'

  # quality checks
  - local: '/.gitlab/ci/coverage.yml'
  - local: '/.gitlab/ci/lint.yml'

  # documentation
  - local: '/.gitlab/ci/docs.yml'

  # deploy packages

# -- global settings ----------------------------

# by default cache everything in the .cache directory
cache:
  key: "${CI_JOB_NAME}"
  paths:
    - .cache

variables:
  PIPELINE_NAME: "LALSuite CI/CD"
  # run autoreconf in pedantic and loud mode
  AUTORECONF: "autoreconf --verbose --warnings=error"
  # version of python to use for builds
  LALSUITE_PYTHON_VERSION: "3.10"
  # gitlab runners have 4 cores per job
  CPU_COUNT: 4
  # don't need git history
  GIT_DEPTH: 1
  # global build helpers
  VERBOSE: "true"
  # where to store files, ideally everything that is 'cache'
  # should go under ${CI_PROJECT_DIR}/.cache to match the
  # cache key setting above
  XDG_CONFIG_HOME: "${CI_PROJECT_DIR}/.config"
  XDG_CACHE_HOME: "${CI_PROJECT_DIR}/.cache"
  CCACHE_DIR: "${CI_PROJECT_DIR}/.cache/ccache"
  CONDA_PKGS_DIRS: "${CI_PROJECT_DIR}/.cache/conda/pkgs"
  PIP_CACHE_DIR: "${CI_PROJECT_DIR}/.cache/pipe"
  # where to build conda environments (i.e. not in ~/.conda)
  CONDA_ENVS_PATH: "${CI_PROJECT_DIR}/envs"
  # what file to use for conda configuration
  CONDARC: "${CI_PROJECT_DIR}/.condarc"
  # tell Debian-based installers to run non-interactively
  DEBIAN_FRONTEND: noninteractive

# default parameters for jobs
default:
  # retry running jobs on failure
  retry: 1
  # cancel job when a newer pipeline starts
  interruptible: true

# -- templates ----------------------------------

.print-env: &print-env
  echo "===== env ====="; env; echo "----- env -----"

.configure-ccache: &configure-ccache
  export PATH=/usr/lib/ccache:/opt/local/libexec/ccache:$PATH

.configure-coredump: &configure-coredump
  ulimit -S -c 0

# basic steps for a job that compiles stuff
# can be included in the before_script/script sections
# via `*init-build-job`
.build-init: &init-build-job
  - *print-env
  - *configure-ccache
  - *configure-coredump

# simple bash command to retry a stage up to 3 times
# with a 15-second delay (mainly to survive transient
# network issues)
.retry: &define-retry |
  retry() {
    local n=1
    local max=3
    local delay=15
    while true; do
      "$@" && break || {
        if [[ $n -lt $max ]]; then
          ((n++))
          echo "Command failed. Attempt $n/$max:"
          sleep $delay;
        else
          echo "The command has failed after $n attempts." 1>&2
          exit 1
        fi
    }
    done
  }

# basic before_script template for a compiling job
.build-job:
  timeout: 3 hours
  before_script:
    - *init-build-job

# a top-level build runs the standard boot/configure/make
# cycle with some sensible defaults
.make-distcheck:
  extends:
    - .build-job
  variables:
    # default configure flags (can be overridden from dependents)
    CONFIGURE_FLAGS: "--enable-doxygen --enable-python --enable-swig-python"
    # default C flags (can be overridden from dependents)
    CFLAGS: "-O3"
    # target to actually run (distcheck is the most thorough)
    MAKE_TARGET: "distcheck"
    # set default python as python3
    # TODO: this can be removed when all packages require python>=3
    #       on their own
    PYTHON: "python3"
  needs: []
  script:
    - ./00boot
    # we use xargs here in case CONFIGURE_FLAGS contains
    # variables with spaces, etc etc
    - xargs ./configure
          --enable-strict-defs
          ${ENABLE_NIGHTLY}
          CFLAGS="${CFLAGS}"
          LALSUITE_LIBTOOL_NO_SUPPRESS=1
          <<< ${CONFIGURE_FLAGS}
    - make -j${CPU_COUNT} VERBOSE=1 ${MAKE_TARGET:-distcheck}
  artifacts:
    # upload some files to debug failures
    paths:
      - config.log
      - Makefile
      - lal*/config.log
      - lal*/libtool
      - lal*/Makefile
      - lal*/**/test-suite.log
    when: on_failure

# helper template for jobs running on macOS
.macos-job:
  variables:
    # use the clone strategy so permissions are correctly reset on any failed jobs
    # see: https://gitlab.com/gitlab-org/gitlab-runner/-/merge_requests/3726
    GIT_STRATEGY: clone
    # Clang has weird bugs when compiling with CFLAGS=-O3
    CFLAGS: "-O2"

# tag for macOS x86_64 architecture jobs
.macos:
  extends:
    - .macos-job
  tags:
    - macos_x86_64

# tag for macOS arm64 architecture jobs
.macos-arm64:
  extends:
    - .macos-job
  tags:
    - macos_arm64

# -- build templates ----------------------------

# jobs that build from the tarballs should not need
# the git repository (this speeds things up and prevents
# other issues)
.build-from-tarball:
  variables:
    GIT_STRATEGY: none

# -- rpm

# template for RPM packaging jobs
.rpmbuild:
  extends:
    - .rhel:rpm
    - .build-from-tarball
  variables:
    # disable repos we don't use
    DISABLE_REPOS: "htcondor osg"
    # parallelise the build (rpmbuild uses this variable by default)
    RPM_BUILD_NCPUS: ${CPU_COUNT}
    # where to build things (not a standard variable name)
    RPM_BUILD_TOPDIR: "${CI_PROJECT_DIR}/rpmbuild"
  # before_script comes from .rhel:rpm
  script:
    - *init-build-job
    # build a yum repo from the upstream packages
    - |
      if [ -d rpmbuild ]; then
          yum install -y -q createrepo
          LOCAL_REPO="${CI_PROJECT_DIR}/local-builds"
          cp -r rpmbuild/RPMS ${LOCAL_REPO}
          createrepo --quiet --workers "${CPU_COUNT}" "${LOCAL_REPO}"
          cat > /etc/yum.repos.d/local-builds.repo <<EOF
      [local-builds]
      name=Local builds
      baseurl=file://${LOCAL_REPO}
      enabled=1
      gpgcheck=0
      EOF
      fi
    # install srpm dependencies
    - yum install -y -q lscsoft-packaging-tools
    # build src.rpm
    - PACKAGE=${CI_JOB_NAME%%:*}
    - cd ${PACKAGE}/
    - TARBALL=$(ls -t1 ${PACKAGE}-*.tar.* | head -n1 | xargs readlink -f)
    - rpmbuild -ts --define "_topdir ${RPM_BUILD_TOPDIR}" ${TARBALL}
    - SRPM=${RPM_BUILD_TOPDIR}/SRPMS/${PACKAGE}-*.src.rpm
    # install build dependencies
    - yum-builddep -y ${SRPM}
    # print installed packages
    - yum list installed --quiet
    # build binary rpms and print details of what we got
    - rpmbuild --rebuild --noclean --define "_topdir ${RPM_BUILD_TOPDIR}" ${SRPM}
    # print package info
    - set +x
    - for rpmf in ${CI_PROJECT_DIR}/rpmbuild/RPMS/*/*${PACKAGE}-*.rpm; do
          echo "===== ${rpmf}" &&
          rpm -qlp "${rpmf}" &&
          echo "Files:" &&
          rpm -qip "${rpmf}" &&
          echo "Provides:" &&
          rpm -qp --provides "${rpmf}" &&
          echo "Requires:" &&
          rpm -qp --requires "${rpmf}";
      done
    # install the packages we built
    - yum -y -q install ${CI_PROJECT_DIR}/rpmbuild/RPMS/*/*${PACKAGE}-*.rpm
    # lint packages (both installed packages and RPM files)
    - |
      cat << EOF > rpmlintrc
      # don't validate Source0
      setOption("NetworkEnabled", False)
      # don't know how to fix this
      addFilter('binary-or-shlib-defines-rpath')
      # the regex rpmlint uses to identify 'lib' libraries is crap
      addFilter('explicit-lib-dependency (.*)?matplotlib')
      addFilter('explicit-lib-dependency (.*)?ciecplib')
      EOF
    - rpmlint
          -f rpmlintrc
          "*${PACKAGE}*"
          ${CI_PROJECT_DIR}/rpmbuild/RPMS/*/*${PACKAGE}-*.rpm
  artifacts:
    expire_in: 18h
    paths:
      # build packages
      - "rpmbuild/RPMS/*/${CI_JOB_NAME%%:*}-*.rpm"
      - "rpmbuild/RPMS/*/lib${CI_JOB_NAME%%:*}-*.rpm"
      - "rpmbuild/RPMS/*/python*-${CI_JOB_NAME%%:*}-*.rpm"
      # log files
      - "rpmbuild/BUILD/**/config.log"
      - "rpmbuild/BUILD/**/test-suite.log"
      # generated sources
      - "rpmbuild/BUILD/**/swiglal_lal*_octave.cpp"
      - "rpmbuild/BUILD/**/swiglal_lal*_python.c"
    reports:
      junit: "rpmbuild/BUILD/**/*junit*.xml"
    when: always
  rules:
    - !reference [.ci-rhel, rules]
    - !reference [.ci-merge-build, rules]
    - !reference [.ci-nightly-deploy, rules]
    - !reference [.ci-lalsuite-tag-build, rules]
    - !reference [.ci-docker, rules]

.rpmbuild:el8:
  extends:
    - .rpmbuild
  image: igwn/base:el8-testing

# -- debian

# template for debian packaging jobs
.debuild:
  extends:
    - .debian:deb
    - .build-from-tarball
  image: igwn/base:bookworm
  variables:
    # tell debhelper to parallelise the build
    DEB_BUILD_OPTIONS: "parallel=${CPU_COUNT}"
  # before_script: comes from .debian:deb
  script:
    - *init-build-job
    - PACKAGE=${CI_JOB_NAME%%:*}
    # install extra build requirements
    - apt-get install -y -q -q
          lintian
    # setup local apt repository to house upstream packages
    - if test -n "$(find . -maxdepth 1 -name '*.deb' -print -quit)"; then
          apt-get -y -q -q install local-apt-repository;
          mkdir -pv /srv/local-apt-repository;
          mv -v *.deb /srv/local-apt-repository;
          /usr/lib/local-apt-repository/rebuild;
          apt-get -y -q update;
      fi
    # create orig tarball
    - cd ${PACKAGE}/
    - TARBALL=$(ls -t1 ${PACKAGE}-*.tar.* | head -n1 | xargs readlink -f)
    - SUFFIX=$(basename $TARBALL | sed 's/.*\.\(tar\..*\)/\1/')
    - VERSION=$(basename $TARBALL | sed 's/[^-]*-\(.*\)\.tar\..*/\1/' | tr '-' '~')
    - cd ${CI_PROJECT_DIR}/
    - cp ${TARBALL} ${PACKAGE}_${VERSION}.orig.${SUFFIX}
    # unpack tarball
    - export DEBFULLNAME="GitLab"
    - export DEBEMAIL="gitlab@git.ligo.org"
    - tar -xf ${TARBALL}
    - cd ${PACKAGE}-*/
    # update changelog
    - dch -v ${VERSION}-1 -b 'Rebuilt automatically on git.ligo.org CI'
    # install build dependencies
    - mk-build-deps
          --tool "apt-get -y -q -o Debug::pkgProblemResolver=yes --no-install-recommends"
          --install
          --remove
    - rm -rfv *-build-deps_*
    # build packages
    - debuild
         --prepend-path=/usr/lib/ccache --set-envvar=CCACHE_DIR=${CCACHE_DIR}
         -us -uc -r
         --lintian-opts --color=always --allow-root
    # print package info
    - set +x
    - cd ${CI_PROJECT_DIR}
    - for debf in *.deb; do
          echo "===== ${debf}";
          dpkg --info "${debf}";
          dpkg --contents "${debf}";
      done
  artifacts:
    expire_in: 18h
    paths:
      # build packages
      - "${CI_JOB_NAME%%:*}*.changes"
      - "${CI_JOB_NAME%%:*}*.deb"
      - "lib${CI_JOB_NAME%%:*}*.deb"
      - "python*-${CI_JOB_NAME%%:*}*.deb"
      - "${CI_JOB_NAME%%:*}*.dsc"
      # log files
      - "${CI_JOB_NAME%%:*}*/**/config.log"
      - "${CI_JOB_NAME%%:*}*/**/test-suite.log"
      # generated sources
      - "${CI_JOB_NAME%%:*}*/**/swiglal_lal*_python.c"
      # the orig tarball
      - "${CI_JOB_NAME%%:*}*.orig.*"
    reports:
      junit: "${CI_JOB_NAME%%:*}-*/**/*junit*.xml"
    when: always
  rules:
    - !reference [.ci-debian, rules]
    - !reference [.ci-merge-build, rules]
    - !reference [.ci-nightly-deploy, rules]
    - !reference [.ci-lalsuite-tag-build, rules]
    - !reference [.ci-docker, rules]

# -- conda

# initialise conda for 'conda build jobs'
# note: this assumes that the base environment is writable,
#       which is not the case by default for the shared
#       macos runners at CIT, if you need a writable base env
#       on those machines, install miniconda/miniforge yourself
.conda-init: &conda-init
  - *define-retry
  # init conda
  - mkdir -p $(dirname ${CONDA_PKGS_DIRS})
  - source ${CONDA_ROOT:=/opt/conda}/etc/profile.d/conda.sh
  # configure conda options
  - rm -fv ${CONDARC}  # start afresh
  - conda config --file ${CONDARC} --set always_yes yes
  - conda config --file ${CONDARC} --add channels conda-forge
  - conda config --file ${CONDARC} --set channel_priority strict
  # see https://github.com/conda-forge/conda-forge-ci-setup-feedstock/pull/168
  - conda config --show aggressive_update_packages >> ${CONDARC}
  - conda config --file ${CONDARC} --remove aggressive_update_packages openssl
  # install build helpers
  - retry conda install -n base
        "conda-build!=3.18.10"
        conda-forge-pinning
        "conda-smithy>=3.7.5"
        conda-libmamba-solver
        conda-verify
  # print info
  - conda activate base
  - conda info --all
  - conda config --show-sources
  - conda config --show
  - conda list --name base

# template for jobs that use conda (in any context)
.conda-job:
  image: igwn/base:conda
  variables:
    # where to write conda packages
    CONDA_BLD_PATH: "${CI_PROJECT_DIR}/conda-bld"
    # dont clone the git repo
    GIT_STRATEGY: none
  before_script:
    - *init-build-job
    - *define-retry
    - *conda-init

# template for `conda build` jobs
.conda-build:
  extends:
    - .conda-job
  variables:
    # stub of feedstock configuration file
    CONDA_CONFIG: "linux_64_"
    # get verbose logging from conda smithy
    CONDA_SMITHY_LOGLEVEL: "DEBUG"
  script:
    - PACKAGE=${CI_JOB_NAME%%:*}
    - cd ${PACKAGE}/
    # copy local packages from conda-bld dir to a new channel
    - if [ -d "${CONDA_BLD_PATH}" ]; then
          LOCAL_CHANNEL="${CI_PROJECT_DIR}/local-builds";
          rm -rf "${LOCAL_CHANNEL}";
          cp -rv "${CONDA_BLD_PATH}" "${LOCAL_CHANNEL}";
          conda index "${LOCAL_CHANNEL}";
          conda config --file ${CONDARC} --add channels "${LOCAL_CHANNEL}";
          conda search "*lal*" --channel "${LOCAL_CHANNEL}" --override-channels;
          rm -rf "${CONDA_BLD_PATH}";
      fi
    # render YAML file to use our tarball
    - TARBALL=$(ls -t1 ${PACKAGE}-*.tar.* | head -n1 | xargs readlink -f)
    - SHA256=$(openssl dgst -r -sha256 $TARBALL | cut -d\  -f1)
    - tar -xf ${TARBALL} --wildcards ${PACKAGE}-*/conda/ --strip-components=1
    - sed 's|@TARBALL@|'${TARBALL}'|g' conda/meta.yaml.in > conda/meta.yaml
    - sed -i 's|@SHA256@|'${SHA256}'|g' conda/meta.yaml
    # create a feedstock from the conda recipe
    - git config --global user.name "${GITLAB_USER_NAME}"
    - git config --global user.email "${GITLAB_USER_EMAIL}"
    - conda smithy init conda/ --feedstock-directory ${PACKAGE}-feedstock
    - cd ${PACKAGE}-feedstock
    # handle migrations that are bundled with the tarball
    - mkdir -p .ci_support/migrations
    - find recipe/migrations -type f -name "*.yaml" -exec cp -n -v {} .ci_support/migrations/ \;
    # regenerate the feedstock
    - retry conda smithy regenerate --no-check-uptodate
    - git ls-files
    # configure CONDA_CONFIG
    - _CONDA_CONFIG_PYTHON_REGEX="(.*python)${LALSUITE_PYTHON_VERSION}(.*)"
    - |
      # if this is a nightly build and CONDA_CONFIG refers to a single python
      # version, expand it to include all Python versions in the feedstock
      if [ ! -z "${ENABLE_NIGHTLY}" ] && [[ "${CONDA_CONFIG}" =~ ${_CONDA_CONFIG_PYTHON_REGEX} ]]; then
        CONDA_CONFIG=$(basename -s .yaml .ci_support/${BASH_REMATCH[1]}*${BASH_REMATCH[2]}.yaml);
      # otherwise if this is NOT a nightly build and CONDA_CONFIG doesn't
      # refer to a single python version, specify `--python` to build only
      # the reference Python version
      elif [ -z "${ENABLE_NIGHTLY}" ] && [[ ! "${CONDA_CONFIG}" =~ ${_CONDA_CONFIG_PYTHON_REGEX} ]]; then
        CONDA_BUILD_ARGS="--python \"${LALSUITE_PYTHON_VERSION}.* *_cpython\"";
      fi
    # ensure $CI_COMMIT_TAG is set for script_env
    - export CI_COMMIT_TAG=${CI_COMMIT_TAG:-}
    # build packages
    # NOTE: we use xargs here because CONDA_BUILD_ARGS contains multiple spaces
    # NOTE: retry if conda build fails due to corrupted $CONDA_PKGS_DIRS
    - |
      # loop over chosen configurations
      for _CONDA_CONFIG in ${CONDA_CONFIG}; do
      # loop over retry
      for n in 1 2; do
        echo "===== conda build: attempt $n of 2 ====="
        if ( \
          set -o pipefail; \
          xargs -t conda build \
            recipe/ \
            --dirty \
            --error-overlinking \
            --keep-old-work \
            --no-anaconda-upload \
            --variant-config-files .ci_support/${_CONDA_CONFIG}.yaml \
            <<< ${CONDA_BUILD_ARGS} \
          2>&1 | awk '{print} /appears to be corrupted/ {exit 1}' \
        ); then
          echo "----- conda build: success -----"
          break
        else
          echo "... output from conda build truncated"
          if [ $n -eq 1 ] && [ "X${CONDA_PKGS_DIRS}" != X ]; then
            echo "----- conda build: possibly ${CONDA_PKGS_DIRS} is corrupted, deleting and retrying -----"
            rm -rf ${CONDA_PKGS_DIRS}
          else
            echo "----- conda build: something else is corrupted, failing -----"
            exit 1
          fi
        fi
      done  # retry
      done  # configs
  after_script:
    # clean cache of old files
    - find ${CONDA_PKGS_DIRS%:*} -atime +30 -delete
    - find ${CONDA_PKGS_DIRS%:*} -type d -empty -delete
  artifacts:
    expire_in: 18h
    paths:
      # built packages (matching this package only)
      - "conda-bld/**/${CI_JOB_NAME%%:*}-*.conda"
      - "conda-bld/**/${CI_JOB_NAME%%:*}-*.tar.bz2"
      - "conda-bld/**/lib${CI_JOB_NAME%%:*}-*.conda"
      - "conda-bld/**/lib${CI_JOB_NAME%%:*}-*.tar.bz2"
      - "conda-bld/**/python-${CI_JOB_NAME%%:*}-*.conda"
      - "conda-bld/**/python-${CI_JOB_NAME%%:*}-*.tar.bz2"
      # log files
      - "conda-bld/${CI_JOB_NAME%%:*}-*/work/**/config.log"
      - "conda-bld/${CI_JOB_NAME%%:*}-*/work/**/test-suite.log"
      # generated sources
      - "conda-bld/${CI_JOB_NAME%%:*}-*/work/**/swiglal_lal*_octave.cpp"
      - "conda-bld/${CI_JOB_NAME%%:*}-*/work/**/swiglal_lal*_python.c"
      # the feedstock
      - "${CI_JOB_NAME%%:*}/${CI_JOB_NAME%%:*}-feedstock/"
    reports:
      # conda-build deletes the _test_tmp directory for each
      # package, so we write them into the project directory
      junit: "*junit*.xml"
    when: always
  rules:
    - !reference [.ci-conda, rules]
    - !reference [.ci-merge-build, rules]
    - !reference [.ci-nightly-deploy, rules]

# -- tarballs -----------------------------------
#
# make tarballs for each subpackage
#

# job template for a subpackage tarball build;
# to build a tarball for a subpackage just define
# a job called `tarball:<subpackage>` that
# `extends` this one (see `tarball:lal` below)
.make-dist:
  image: igwn/lalsuite-dev:bookworm
  stage: tarballs
  needs: []
  extends:
    - .build-job
  script:
    - pushd ${CI_JOB_NAME##*:}
    - ./00boot
    - ./configure ${ENABLE_NIGHTLY}
    - make dist
  artifacts:
    expire_in: 18h
    # store the tarballs
    paths:
      - "*/*.tar.*"
    # there are no reports for tarball jobs
    reports:
      junit: []

# make tarballs for _all_ packages
tarballs:
  extends:
    - .make-dist
  script:
    - ./00boot
    - ./configure ${ENABLE_NIGHTLY}
    - for subdir in lal lalframe lalmetaio lalsimulation lalburst lalinspiral lalinference lalpulsar lalapps; do
        pushd ${subdir};
        make dist;
        popd;
      done

# make the tarball for LAL only
# (this job will run much faster than the `tarballs` job
#  so we use it to release the 'LAL' stage jobs as
#  early as possible)
tarball:lal:
  extends:
    - .make-dist

# -- lal ----------------------------------------
#
# build packages for LAL
#

.lal:
  stage: LAL
  needs:
    - tarball:lal

lal:rpm-el8:
  extends:
     - .rpmbuild:el8
     - .lal

lal:deb:
  extends:
    - .debuild
    - .lal

# build with conda using FFTW
lal:conda:fftw:
  extends:
    - .conda-build
    - .lal
  variables:
    CONDA_CONFIG: "linux_64_fft_implfftw"

# build with conda using Intel FFT (MKL)
lal:conda:mkl:
  extends:
    - .conda-build
    - .lal
  variables:
    CONDA_CONFIG: "linux_64_fft_implmkl"

# -- lalframe------------------------------------
#
# build packages for LALFrame
#

.lalframe:
  stage: LALFrame

lalframe:rpm-el8:
  extends:
    - .rpmbuild:el8
    - .lalframe
  needs:
    - tarballs
    - lal:rpm-el8

lalframe:deb:
  extends:
    - .debuild
    - .lalframe
  needs:
    - tarballs
    - lal:deb

lalframe:conda:
  extends:
    - .conda-build
    - .lalframe
  needs:
    - tarballs
    - lal:conda:fftw

# -- lalmetaio ----------------------------------
#
# build packages for LALMetaIO
#

.lalmetaio:
  stage: LALMetaIO

lalmetaio:rpm-el8:
  extends:
    - .rpmbuild:el8
    - .lalmetaio
  needs:
    - tarballs
    - lal:rpm-el8

lalmetaio:deb:
  extends:
    - .debuild
    - .lalmetaio
  needs:
    - tarballs
    - lal:deb

lalmetaio:conda:
  extends:
    - .conda-build
    - .lalmetaio
  needs:
    - tarballs
    - lal:conda:fftw

# -- lalsimulation ------------------------------
#
# build packages for LALSimulation
#

.lalsimulation:
  stage: LALSimulation

lalsimulation:rpm-el8:
  extends:
    - .rpmbuild:el8
    - .lalsimulation
  needs:
    - tarballs
    - lal:rpm-el8

lalsimulation:deb:
  extends:
    - .debuild
    - .lalsimulation
  needs:
    - tarballs
    - lal:deb

lalsimulation:conda:
  extends:
    - .conda-build
    - .lalsimulation
  needs:
    - tarballs
    - lal:conda:fftw
  variables:
    CONDA_CONFIG: "linux_64_*python${LALSUITE_PYTHON_VERSION}.*cpython"

# -- lalburst -----------------------------------
#
# build packages for LALBurst
#

.lalburst:
  stage: LALBurst

lalburst:rpm-el8:
  extends:
    - .rpmbuild:el8
    - .lalburst
  needs:
    - tarballs
    - lal:rpm-el8
    - lalmetaio:rpm-el8
    - lalsimulation:rpm-el8

lalburst:deb:
  extends:
    - .debuild
    - .lalburst
  needs:
    - tarballs
    - lal:deb
    - lalmetaio:deb
    - lalsimulation:deb

lalburst:conda:
  extends:
    - .conda-build
    - .lalburst
  needs:
    - tarballs
    - lal:conda:fftw
    - lalmetaio:conda
    - lalsimulation:conda

# -- lalinspiral --------------------------------
#
# build packages for LALInspiral
#

.lalinspiral:
  stage: LALInspiral

lalinspiral:rpm-el8:
  extends:
    - .rpmbuild:el8
    - .lalinspiral
  needs:
    - tarballs
    - lal:rpm-el8
    - lalframe:rpm-el8
    - lalmetaio:rpm-el8
    - lalsimulation:rpm-el8
    - lalburst:rpm-el8

lalinspiral:deb:
  extends:
    - .debuild
    - .lalinspiral
  needs:
    - tarballs
    - lal:deb
    - lalframe:deb
    - lalmetaio:deb
    - lalsimulation:deb
    - lalburst:deb

lalinspiral:conda:
  extends:
    - .conda-build
    - .lalinspiral
  needs:
    - tarballs
    - lal:conda:fftw
    - lalframe:conda
    - lalmetaio:conda
    - lalsimulation:conda
    - lalburst:conda

# -- lalinference -------------------------------
#
# build packages for LALInference
#

.lalinference:
  stage: LALInference

lalinference:rpm-el8:
  extends:
    - .rpmbuild:el8
    - .lalinference
  needs:
    - tarballs
    - lal:rpm-el8
    - lalframe:rpm-el8
    - lalmetaio:rpm-el8
    - lalsimulation:rpm-el8
    - lalburst:rpm-el8
    - lalinspiral:rpm-el8

lalinference:deb:
  extends:
    - .debuild
    - .lalinference
  needs:
    - tarballs
    - lal:deb
    - lalframe:deb
    - lalmetaio:deb
    - lalsimulation:deb
    - lalburst:deb
    - lalinspiral:deb

lalinference:conda:
  extends:
    - .conda-build
    - .lalinference
  needs:
    - tarballs
    - lal:conda:fftw
    - lalframe:conda
    - lalmetaio:conda
    - lalsimulation:conda
    - lalburst:conda
    - lalinspiral:conda

# -- lalpulsar ----------------------------------
#
# build packages for LALPulsar
#

.lalpulsar:
  stage: LALPulsar

lalpulsar:rpm-el8:
  extends:
    - .rpmbuild:el8
    - .lalpulsar
  needs:
    - tarballs
    - lal:rpm-el8
    - lalframe:rpm-el8
    - lalmetaio:rpm-el8
    - lalsimulation:rpm-el8
    - lalburst:rpm-el8
    - lalinspiral:rpm-el8
    - lalinference:rpm-el8

lalpulsar:deb:
  extends:
    - .debuild
    - .lalpulsar
  needs:
    - tarballs
    - lal:deb
    - lalframe:deb
    - lalmetaio:deb
    - lalsimulation:deb
    - lalburst:deb
    - lalinspiral:deb
    - lalinference:deb

lalpulsar:conda:
  extends:
    - .conda-build
    - .lalpulsar
  needs:
    - tarballs
    - lal:conda:fftw
    - lalframe:conda
    - lalmetaio:conda
    - lalsimulation:conda
    - lalburst:conda
    - lalinspiral:conda
    - lalinference:conda

# -- lalapps ------------------------------------
#
# build packages for LALApps
#

.lalapps:
  stage: LALApps

lalapps:rpm-el8:
  extends:
    - .rpmbuild:el8
    - .lalapps
  needs:
    - tarballs
    - lal:rpm-el8
    - lalframe:rpm-el8
    - lalmetaio:rpm-el8
    - lalsimulation:rpm-el8
    - lalburst:rpm-el8
    - lalinspiral:rpm-el8
    - lalinference:rpm-el8
    - lalpulsar:rpm-el8

lalapps:deb:
  extends:
    - .debuild
    - .lalapps
  needs:
    - tarballs
    - lal:deb
    - lalframe:deb
    - lalmetaio:deb
    - lalsimulation:deb
    - lalburst:deb
    - lalinspiral:deb
    - lalinference:deb
    - lalpulsar:deb

lalapps:conda:
  extends:
    - .conda-build
    - .lalapps
  needs:
    - tarballs
    - lal:conda:fftw
    - lalframe:conda
    - lalmetaio:conda
    - lalsimulation:conda
    - lalburst:conda
    - lalinspiral:conda
    - lalinference:conda
    - lalpulsar:conda
  variables:
    CONDA_CONFIG: "linux_64_*python${LALSUITE_PYTHON_VERSION}.*cpython"

# -- wheels -------------------------------------
#
# build python wheels
#

# job template for wheel builds
.wheel:
  stage: wheels
  needs: []
  extends:
    - .build-job
  rules:
    - !reference [.ci-wheels, rules]
    - !reference [.ci-lalsuite-tag-build, rules]
    - !reference [.ci-nightly-deploy, rules]
  artifacts:
    expire_in: 18h
    paths:
      - wheelhouse
    when: always

# do some simple sanity checks in a virtualenv
.wheel-test: &wheel-test
  # remove build tree first to make sure wheel does not
  # access it, e.g. try to find LAL data files there
  - rm -rf wheel/build/
  # install wheel
  - ${PYTHON} -m venv test
  - source test/bin/activate
  - python -m pip install --upgrade pip
  - python -m pip install solar_system_ephemerides
  - python -m pip install wheelhouse/*
  # check metadata
  - python -m pip show lalsuite
  - python -m pip check lalsuite
  # test loading Python modules and finding data files
  - |
    env LAL_DEBUG_LEVEL=info python3 - <<EOF
    import lal
    import lalframe
    import lalmetaio
    import lalsimulation
    import lalpulsar
    series = lal.CreateREAL8FrequencySeries('psd', 0, 0, 1, None, 4096)
    lalsimulation.SimNoisePSDaLIGOAPlusDesignSensitivityT1800042(series, 10)
    lalpulsar.InitBarycenter("earth00-40-DE405.dat.gz", "sun00-40-DE405.dat.gz")
    EOF
  # test running C executables and finding data files
  - lalapps_version
  - env LAL_DEBUG_LEVEL=info lalsim-detector-noise --official --aligo-nsnsopt --duration 1 >/dev/null
  - env LAL_DEBUG_LEVEL=info lalpulsar_PrintDetectorState --detector H1 --Alpha 4.65 --Delta -0.51 --timeGPS 1100000000 >/dev/null

# Build receipe for standalone wheels on Linux
.wheel:manylinux:
  extends:
    - .wheel
  script:
    - export PYTHON="/opt/python/$(echo ${CI_JOB_NAME} | sed 's/.*:\(.*\)-manylinux.*/\1/')/bin/python"
    # Build against numpy>=2.0.0rc1 for ABI compatibility with Numpy 1.x and 2.x.
    # See https://numpy.org/devdocs/dev/depending_on_numpy.html#numpy-2-abi-handling.
    - ${PYTHON} -m pip install --upgrade pip
    - ${PYTHON} -m pip install 'numpy>=2.0.0rc1'
    # Build wheel
    - ./00boot
    - SSE="solar_system_ephemerides/ephemerides"
    - ./configure
          ${ENABLE_NIGHTLY}
          --with-fallback-data-path="../lalapps/data:../${SSE}/earth:../${SSE}/sun:../${SSE}/time"
          --disable-doxygen
          --enable-mpi
          --enable-python
          --enable-swig-python
          --without-ephem
          PYTHON=${PYTHON}
    - make -j${CPU_COUNT} wheel
    # Bundle and fix up dependent shared libraries
    - auditwheel repair wheel/*.whl
    # Test: do some simple sanity checks in a virtualenv
    - *wheel-test

# Build receipe for standalone wheels on macOS
.wheel:macos:
  extends:
    - .wheel
    - .macos-job
  script:
    # this build uses macports for the libraries and Python
    - . /opt/local/share/macports/setupenv.bash
    # find python
    - PYTHON=$(which python$(echo ${CI_JOB_NAME} | sed 's/.*:cp\([0-9]\)\([0-9]\{1,\}\).*/\1.\2/'))
    # Enter virtualenv so that we have a controlled version of Numpy.
    # Build against numpy>=2.0.0rc1 for ABI compatibility with Numpy 1.x and 2.x.
    # See https://numpy.org/devdocs/dev/depending_on_numpy.html#numpy-2-abi-handling.
    - ${PYTHON} -m venv env
    - source env/bin/activate
    - python -m pip install --upgrade pip
    - python -m pip install
          delocate
          'numpy>=2.0.1'
          pip
          setuptools
          wheel
    - python -m pip list installed
    # Build wheel
    - ./00boot
    - SSE="solar_system_ephemerides/ephemerides"
    - ./configure
          ${ENABLE_NIGHTLY}
          --with-fallback-data-path="../../lalapps/data:../../${SSE}/earth:../../${SSE}/sun:../../${SSE}/time"
          --disable-doxygen
          --enable-mpi
          --enable-python
          --enable-swig-python
          --without-ephem
          PYTHON=$(which python)
          LDFLAGS=-Wl,-headerpad_max_install_names
    - make -j${CPU_COUNT} wheel
    # Bundle and fix up dependent shared libraries
    - delocate-wheel -v -w wheelhouse wheel/*.whl
    # Test: do some simple sanity checks in a virtualenv
    - *wheel-test

.wheel:macos:x86_64:
  tags:
    # explicitly use the oldest macOS tag we have
    - macos_bigsur_x86_64

.wheel:macos:arm64:
  tags:
    # explicitly use the oldest macOS tag we have
    - macos_monterey_arm64

# wheel build using the manylinux_2_28_x86_64 container
.wheel:manylinux_2_28_x86_64:
  extends:
    - .wheel:manylinux
  image: containers.ligo.org/lscsoft/lalsuite-manylinux/manylinux_2_28_x86_64

# wheel build using the manylinux_2_28_aarch64 container
.wheel:manylinux_2_28_aarch64:
  extends:
    - .wheel:manylinux
  image: containers.ligo.org/lscsoft/lalsuite-manylinux/manylinux_2_28_aarch64
  tags:
    - aarch64

# Build wheels for all supported platforms
wheel:cp39-cp39-manylinux_2_28_x86_64:
  extends:
    - .wheel:manylinux_2_28_x86_64
wheel:cp310-cp310-manylinux_2_28_x86_64:
  extends:
    - .wheel:manylinux_2_28_x86_64
wheel:cp311-cp311-manylinux_2_28_x86_64:
  extends:
    - .wheel:manylinux_2_28_x86_64
wheel:cp312-cp312-manylinux_2_28_x86_64:
  extends:
    - .wheel:manylinux_2_28_x86_64
  rules:  # reference build for this platform
    - !reference [.ci-wheels, rules]
    - !reference [.ci-merge-build, rules]
    - !reference [.ci-lalsuite-tag-build, rules]
    - !reference [.ci-nightly-deploy, rules]
wheel:cp39-cp39-manylinux_2_28_aarch64:
  extends:
    - .wheel:manylinux_2_28_aarch64
wheel:cp310-cp310-manylinux_2_28_aarch64:
  extends:
    - .wheel:manylinux_2_28_aarch64
wheel:cp311-cp311-manylinux_2_28_aarch64:
  extends:
    - .wheel:manylinux_2_28_aarch64
wheel:cp312-cp312-manylinux_2_28_aarch64:
  extends:
    - .wheel:manylinux_2_28_aarch64
  rules:  # reference build for this platform
    - !reference [.ci-wheels, rules]
    # Disable building aarch64 for MRs due to low availability of aarch64 runners
    # - !reference [.ci-merge-build, rules]
    - !reference [.ci-lalsuite-tag-build, rules]
    - !reference [.ci-nightly-deploy, rules]
wheel:cp39-cp39-macos-x86_64:
  extends:
    - .wheel:macos
    - .wheel:macos:x86_64
wheel:cp310-cp310-macos-x86_64:
  extends:
     - .wheel:macos
     - .wheel:macos:x86_64
wheel:cp311-cp311-macos-x86_64:
  extends:
     - .wheel:macos
     - .wheel:macos:x86_64
wheel:cp312-cp312-macos-x86_64:
  extends:
     - .wheel:macos
     - .wheel:macos:x86_64
  rules:  # reference build for this platform
    - !reference [.ci-wheels, rules]
    - !reference [.ci-merge-build, rules]
    - !reference [.ci-lalsuite-tag-build, rules]
    - !reference [.ci-nightly-deploy, rules]
wheel:cp39-cp39-macos-arm64:
  extends:
    - .wheel:macos
    - .wheel:macos:arm64
wheel:cp310-cp310-macos-arm64:
  extends:
     - .wheel:macos
     - .wheel:macos:arm64
wheel:cp311-cp311-macos-arm64:
  extends:
     - .wheel:macos
     - .wheel:macos:arm64
wheel:cp312-cp312-macos-arm64:
  extends:
     - .wheel:macos
     - .wheel:macos:arm64
  rules:  # reference build for this platform
    - !reference [.ci-wheels, rules]
    - !reference [.ci-merge-build, rules]
    - !reference [.ci-lalsuite-tag-build, rules]
    - !reference [.ci-nightly-deploy, rules]

# -- deploy -------------------------------------
#
# Deploy outputs to various locations
#

.deploy:
  stage: deploy
  variables:
    GIT_STRATEGY: none
  retry: 0
  interruptible: false

# deploy wheels
wheel:pypi:
  extends:
    - .deploy
  image: python
  script:
    # TWINE_USERNAME and TWINE_PASSWORD are provided by CI secret variables
    - pip install --upgrade pip
    - pip install twine
    - |
      if [ "X${EXECUTE_DEPLOY_ACTIONS}" != "Xyes" ]; then
        echo "Skipping rest of job as EXECUTE_DEPLOY_ACTIONS!=yes"
        exit 100
      fi
    - twine upload wheelhouse/*
  needs:
    - wheel:cp39-cp39-manylinux_2_28_x86_64
    - wheel:cp310-cp310-manylinux_2_28_x86_64
    - wheel:cp311-cp311-manylinux_2_28_x86_64
    - wheel:cp312-cp312-manylinux_2_28_x86_64
    - wheel:cp39-cp39-manylinux_2_28_aarch64
    - wheel:cp310-cp310-manylinux_2_28_aarch64
    - wheel:cp311-cp311-manylinux_2_28_aarch64
    - wheel:cp312-cp312-manylinux_2_28_aarch64
    - wheel:cp39-cp39-macos-x86_64
    - wheel:cp310-cp310-macos-x86_64
    - wheel:cp311-cp311-macos-x86_64
    - wheel:cp312-cp312-macos-x86_64
    - wheel:cp39-cp39-macos-arm64
    - wheel:cp310-cp310-macos-arm64
    - wheel:cp311-cp311-macos-arm64
    - wheel:cp312-cp312-macos-arm64
  rules:
    - !reference [.ci-lalsuite-tag-build, rules]
    - !reference [.ci-nightly-deploy, rules]
  allow_failure:
    exit_codes: 100   # EXECUTE_DEPLOY_ACTIONS!=yes
