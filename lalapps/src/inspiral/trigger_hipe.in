#!@PYTHONPROG@
# coding=utf-8
"""
trigger_hipe.in - external-trigger inspiral pipeline driver script

$Id$

This script uses master segment lists to determine a set of segment
lists appropriate to running inspiral_hipe on time around a GRB. At
present, the script only sets up directories and writes appropriate
segment files to those directories.

It uses the same configuration file as the inspiral_hipe script to
determine various parameters and then set up analysis and injection
runs.

"""
from __future__ import division

__author__ = 'Patrick Brady <patrick@gravity.phys.uwm.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

##############################################################################
import copy
import glob
import math
import sha
import os
import sys
import ConfigParser
import optparse

import numpy

from glue.ligolw import ligolw
from glue.ligolw import utils
from glue.ligolw import table
from glue.ligolw import lsctables
from glue import segments
from glue import segmentsUtils
from glue import pipeline
from pylal import grbsummary
from pylal import dqutils
from lalapps import inspiralutils

##############################################################################
# define a few utility functions that make this job easier

# subroutine to rescale the times
def ct(time, timeOffset):
  return (time-timeOffset)  # /3600.0


def mkdir( directory, overwrite ):
  try:
    os.mkdir(directory)
  except OSError:
    if overwrite:
      print "Warning: Overwriting contents in existing directory %s" % directory
    else:
      raise

def hash_n_bits(input_string, n_bits, hash_lib=sha):
  """
  Return the first n_bits (as an integer) of a hash on input_string.  Defaults
  to SHA-1.
  
  n_bits=31 is useful for, say, random seeds that must fit into a LAL INT4.
  """
  # Some test examples:
  #
  # In [30]: hex(hash_n_bits("test", 28) << (32 - 28))
  # Out[30]: '0xa94a8fe0L'
  # 
  # In [31]: hex(hash_n_bits("test", 29) << (32 - 29))
  # Out[31]: '0xa94a8fe0L'
  # 
  # In [32]: hex(hash_n_bits("test", 30) << (32 - 30))
  # Out[32]: '0xa94a8fe4L'
  # 
  # In [33]: hex(hash_n_bits("test", 31) << (32 - 31))
  # Out[33]: '0xa94a8fe4L'
  # 
  # In [34]: hex(hash_n_bits("test", 32))
  # Out[34]: '0xa94a8fe5L'
  
  hashed_string = hash_lib.new(input_string).digest()
  n_bytes = int(math.ceil(n_bits / 8))
  
  def bytes2int(byte_string):
    result = 0
    for byte in byte_string:
      result <<= 8
      result += ord(byte)
    return result
  
  return bytes2int(hashed_string[:n_bytes]) >> (8*n_bytes - n_bits)

def createInjectionFile(hipe_dir, cp, cpinj, injrun, injection_segment,
  source_file, verbose=False):
  """
  Creates an master injection file containing all injections for this run.
  Also reads the file and returns its contents
  """
  cpinj = copy.deepcopy(cpinj)

  # get the number of injections to be made
  for opt in ['exttrig-inj-start','exttrig-inj-stop']:
    value = int(cpinj.get(injrun,opt))
    cpinj.remove_option(injrun,opt)
    if 'start' in opt:
      injStart = value
    else: 
      injEnd = value
  seed = hash_n_bits(hipe_dir, 31)
  numberInjections = injEnd - injStart + 1  # e.g., 1 through 5000 inclusive

  # set all the arguments
  argument = []
  for (opt,value) in cpinj.items(injrun):
    argument.append("--%s %s" % (opt, value) )

  # add arguments on times and time-intervals
  interval = abs(injection_segment)
  injInterval = interval / numberInjections
  argument.append(" --gps-start-time %d" % injection_segment[0] )
  argument.append(" --gps-end-time %d" % injection_segment[1] )
  argument.append(" --time-interval %f" % injInterval )
  argument.append(" --time-step %f" % injInterval )
  argument.append(" --seed %d" % seed )

  # set output file and exttrig-file
  injFile = "%s/HL-INJECTIONS.xml" % hipe_dir
  argument.append(" --output %s" % injFile )
  argument.append(" --exttrig-file %s" % source_file )

  # execute the command
  executable = cp.get("condor", "inspinj")
  arguments = " ".join(argument)
  inspiralutils.make_external_call(executable + " " + arguments,
    show_stdout=verbose, show_command=verbose)

  # read in the file and the tables
  doc = utils.load_filename( injFile )    
  sims = table.get_table(doc, lsctables.SimInspiralTable.tableName)

  return sims, injInterval, numberInjections

def fix_numrel_columns(sims):
   """
   Due to a mismatch between our LAL tag and the LAL HEAD, against which
   we compile pylal, there are missing columns that cannot be missing.
   Just put in nonsense values so that glue.ligolw code won't barf.
   """
   for sim in sims:
       sim.numrel_data="nan"
       sim.numrel_mode_max = 0
       sim.numrel_mode_min = 0

def plot_segments(trigger, ifolist, segdict, onSourceSegment, offSourceSegment, subdir):
    """
    Plot the on- and off-source segment extents for all IFOs in ifolist.
    """
    colorCode = {'H1':'r', 'H2':'b', 'L1':'g', 'V1':'m', 'G1':'k'}
    hi = len(ifolist)

    fig = pylab.figure()
    ax = fig.add_subplot(111)
    ax.plot([-6, 6], [0, hi], 'w.')
    area = {}
    for c, ifo in enumerate(ifolist):
      color = colorCode[ifo]
      for seg in segdict[ifo]:
        a = ct(seg[0], trigger)
        b = ct(seg[1], trigger)
        ax.fill([a, b, b, a, a], [c, c, c+1, c+1, c], color)
    ax.axvline(ct(offSourceSegment[0], trigger), color='k', lw=4)
    ax.axvline(ct(offSourceSegment[1], trigger), color='k', lw=4)
    ax.axvline(ct(onSourceSegment[0], trigger), color='k', ls='--')
    ax.axvline(ct(onSourceSegment[1], trigger), color='k', ls='--')
    ax.set_xlabel('time [h]')
    ax.set_ylabel('IFO')
    
    a = ct(offSourceSegment[0] - 2 * minsciseg, trigger)
    b = ct(offSourceSegment[1] + 2 * minsciseg, trigger)
    ticks = numpy.arange(hi) + 0.5
    ax.set_yticks(ticks)
    ax.set_yticklabels(ifolist)
    for x in ax.get_xticks():
      ax.plot([x, x], [0, hi], 'k:')
    for y in range(hi):
      ax.plot([-6,6], [y,y], 'k:')
    ax.set_xlim([a, b])
    
    fig.savefig(subdir+"/segments-"+subdir+".png")
    pylab.close(fig)

def plot_injection_distributions(sims, tag, subdir, overwrite_flag=False):
    """
    Plot histograms of all injected parameters that have numeric values.
    Create directory tag+"_injection" and fill with *_distribution.png.
    """ 
    plot_dir = tag + "_injection_distributions"
    
    # temporarily change to directory
    cwd = os.getcwdu()
    os.chdir(subdir)
    mkdir(plot_dir, overwrite_flag)
    os.chdir(plot_dir)
    
    for attr, attr_type in zip(sims.columnnames, sims.columntypes):
        if attr_type.startswith("int") or attr_type.startswith("real"):
            fig = pylab.figure()
            ax = fig.add_subplot(111)
            ax.hist([getattr(sim, attr) for sim in sims], bins=20)
            ax.set_xlabel(attr.replace("_", r"\_"))
            ax.set_ylabel(r"\# of injections")
            fig.savefig(str(attr) + "_distribution.png")
            pylab.close(fig)

    # return to original directory
    os.chdir(cwd)

class hipe_run(object):
  """
  This class is intended to represent single run of lalapps_inspiral_hipe.
  """
  def __init__(self, hipe_dir, base_cp, ifos, log_path, source_file, dag_name,
               span, exttrig_analyze=None, usertag=None, verbose=False,
               only_run=None, dont_run=None):
    self._hipe_dir = hipe_dir
    self._log_path = log_path
    self._cp = copy.deepcopy(base_cp)
    self._ifos = ifos
    self._verbose = verbose
    self._dag_name = os.path.join(self._hipe_dir, dag_name)
    self._usertag = usertag
    self._only_run = only_run
    self._dont_run = dont_run
    self._span = span
    
    # create directory here
    mkdir(self._hipe_dir, overwrite=True)
 
    ## Determine arguments to inspiral_hipe
    self._hipe_args = ["--output-segs", "--log-path=%s" % log_path,
      "--config-file=%s", "--write-script"]
    
    # decide what stages to run
    allowed_stages = set(["datafind", "template-bank", "inspiral",
      "coincidence", "trigbank", "inspiral-veto", "second-coinc", 
      "coire-second-coinc", "sire-inspiral"])
    if only_run is not None:
      if not (set(only_run) <= allowed_stages):
        raise ValueError, "stage not in allowed stages"
      for stage in only_run:
        self._hipe_args.append("--" + stage)
    else:
      if dont_run is not None:
        stages = allowed_stages - set(dont_run)
      for stage in stages:
        self._hipe_args.append("--" + stage)

    
    # determine how many ifos to analyze
    n = len(self._ifos)
    if n < 1 or n > 4:
      raise ValueError, "cannot handle less than one or more than four IFOs"
    number_words = {1: "one", 2: "two", 3: "three", 4: "four"}
    for i in range(n):
      self._hipe_args.append("--%s-ifo" % number_words[i+1])
    
    # set individual ifos to analyze
    for ifo in self._ifos:
      self._cp.set("input", "%s-segments" % ifo.lower(), "../offSourceSeg.txt")
      self._hipe_args.append("--%s-data" % ifo.lower())

    # set the source file for thinca
    self._cp.set('thinca','exttrig','../../'+source_file)

    # set the on-source segment
    if exttrig_analyze is not None:
      self._cp.set('input','exttrig-segments', '../onSourceSeg.txt')
      self._cp.set('input','exttrig-analyze', exttrig_analyze)
    else:
      self._cp.set('input', 'exttrig-analyze', 'all')
    
    # set the usertag
    if usertag is not None:
      self._cp.set("pipeline", "user-tag", usertag)
    
    # set start and end times for cache filenames
    self._cp.set("input", "gps-start-time", self._span[0])
    self._cp.set("input", "gps-end-time", self._span[1])

  def set_numslides(self, numslides):
    if numslides == 0: numslides = ""
    self._cp.set('input', 'num-slides', numslides)

  def set_injections(self, injrun, numberInjFiles):
    """
    Turn this analysis into an injection run, using the injrun section from
    the config file cpinj.
    """
    # set the start and stop seeds from the injection config file
    self._cp.set('pipeline', 'exttrig-inj-start', 1)
    self._cp.set('pipeline', 'exttrig-inj-stop', numberInjFiles)
    self._cp.set("condor", "inspinj", "/bin/true")

  def write_dag(self):
    """
    Create directory and run lalapps_inspiral_hipe to create the DAG.
    """
    # write out an appropriate ini file
    ini_file_name = os.path.basename( self._dag_name+'.ini' )
    os.chdir(self._hipe_dir)
    if not os.path.isdir(self._log_path):  # in case of relative path
      os.mkdir(self._log_path)
    cp_file = open( ini_file_name, 'w')
    self._cp.write(cp_file)
    cp_file.close()
    arguments = " ".join(self._hipe_args)
    
    # run inspiral_hipe
    inspiralutils.make_external_call("../../lalapps_inspiral_hipe %s" % (arguments % ini_file_name),
      show_stdout=False, show_command=self._verbose)
    
    # create link to datafind directory's cache directory
    if self._only_run != ["datafind"]:
      if os.path.isdir("cache") and not os.path.islink("cache"):
        os.rmdir("cache")
      if not os.path.exists("cache"):
        os.symlink("../datafind/cache", "cache")
    
    # set inspinj jobs to run in the local universe
    inspinj_sub_files = glob.glob("*inspinj*.sub")
    for sub_file in inspinj_sub_files:
      contents = open(sub_file).read().replace("universe = standard", "universe = local")
      open(sub_file, "w").write(contents)
    
    os.chdir("../../")
  
  def get_cache_name(self):
    return self._hipe_dir + "/" + \
      inspiralutils.hipe_cache(self._ifos, self._usertag, self._span[0],
                               self._span[1])
  
  def get_dag_name(self):
    dag_name = self._dag_name
    
    # match inspiral_hipe's convention
    if self._usertag is not None:
      dag_name += "." + self._usertag
    
    return dag_name + '.dag'
  
  def get_dag_node(self):
    """
    Return a tuple of (job, node), where these refer to this hipe_run's DAG.
    """
    job = pipeline.CondorDAGManJob(self.get_dag_name())
    node = pipeline.CondorDAGNode(job)
    
    # hack to make rescue possible
    inspiralutils.fix_rescue(node)
    return node

def concatenate_cache_files(caches, output_cache):
  """
  Get the cache outputs from each hipe_run and concatenate them to a file
  named output_cache.
  """
  inspiralutils.make_external_call("cat %s > %s" % \
                                   (" ".join(caches), output_cache))

##############################################################################
# define usage and command line options and arguments - parse
usage = """usage: %prog ...

Lay down a directory hierarchy appropriate to analyzing the data
around the time of GRB using the inspiral pipeline. It determines an
appropriate amount of data on each side of the reported GRB trigger.

As of now, the code appears to do the right thing.  This code also
uses the inspiral_hipe config file to determine information about
segments and to ensure that appropriate overlaps, etc are being done.  


DIRECTORY HIERARCHY:

The directory hierarchy that a search would have then follows:

searchdir
  GRB<grb.number>
    onsource
    offsource
    injectionNNN
    injectionNNN
    ....
    injectionNNN
  .
  .
  .

METADATA FILES:

The script currently writes ....


RELATED TOOLS AND REQUIRED TOOLS:

With a structure like this, a host of other tools can be developed to
make the whole analysis engine work well. Here is a list of things
that we need with a note about its current status:

* inspiral_hipe:  exists and meta-stable
* grb_hipe: exists, but developmental
* grb_summary: exists, but developmental

"""
parser = optparse.OptionParser(usage, version="$Id$")

parser.add_option("-v", "--verbose", action="store_true",default=False,\
  help="make things verbose" )
  
# what data?
parser.add_option("-G","--g1-segments",action="store",type="string",\
  default=None, metavar=" G1_SEGMENTS", help="G1 input segment to read" )
parser.add_option("-H","--h1-segments",action="store",type="string",\
  default=None, metavar=" H1_SEGMENTS", help="H1 input segment to read" )
parser.add_option("-K","--h2-segments",action="store",type="string",\
  default=None, metavar=" H2_SEGMENTS", help="H2 input segment to read" )
parser.add_option("-L","--l1-segments",action="store",type="string",\
  default=None, metavar=" L1_SEGMENTS", help="L1 input segment to read" )
parser.add_option("-V","--v1-segments",action="store",type="string",\
  default=None, metavar=" V1_SEGMENTS", help="V1 input segment to read" )
parser.add_option("-T","--grb",action="append",type="string",\
    default=["all"], metavar=" GRB NAME",\
    help="name of the GRB to be analyzed")
parser.add_option("-l","--list",action="store",type="string",\
    default=None, metavar=" GRB LIST",\
    help="path to the file with GRB data stored")

# what parameters?
parser.add_option("-a","--onsource-left",action="store",type="int",\
    default=120, metavar=" ONLEFT",\
    help="Specifies the left onsource time window (default: 120)")
parser.add_option("-b","--onsource-right",action="store",type="int",\
    default=60, metavar=" ONRIGHT",\
    help="Specifies the right onsource time window (default:60)")
parser.add_option("-t", "--padding-time", type="int", default=0,
  help="padding time on each side of an analysis segment")
parser.add_option("-n", "--num-trials", type="int",
  help="the off-source segment should contain NUM_TRIALS segments of the length of the on-source segment")
parser.add_option("-s", "--symmetric", action="store_true", default=False,
  help="restrict the off-source trials to be arranged symmetrically about "\
       "the on-source segment (default: asymmetrical distribution allowed)")
parser.add_option("-o", "--overwrite-dir", action="store_true",default=False,\
  help="overwrite contents in already existing directories" )

# read in the config file
parser.add_option("-f","--config-file",action="store",type="string",\
  default=None, metavar=" FILE", help="use configuration file FILE" )
parser.add_option("-g","--injection-config",action="store",type="string",\
  default=None, metavar=" FILE", help="use configuration file FILE" )
parser.add_option("-p", "--log-path",action="store",type="string",\
    metavar=" PATH",help="directory to write condor log file")

# Add some extra capabilities
parser.add_option("-P", "--plot-segments", action="store_true",default=False,\
  help="plot segments for each interval with original segments" )
parser.add_option("-D", "--plot-injections", action="store_true",
  default=False, help="histogram distributions of each injection parameter")
parser.add_option("", "--skip-dataquality", action="store_false", default=True,
  dest="do_dataquality", help="skip a query for data quality")

( opts , args ) = parser.parse_args()

required_opts = ["list", "onsource_right", "onsource_left", "padding_time",
    "num_trials", "config_file", "injection_config", "log_path"]
for opt in required_opts:
    if getattr(opts, opt) is None:
        raise ValueError, "--%s is a required option" % opt

if opts.plot_segments and opts.plot_injections:
  import matplotlib
  matplotlib.use("Agg")
  import pylab

##############################################################################
# create the config parser object and read in the ini file
cp = ConfigParser.ConfigParser()
cp.read(opts.config_file)

##############################################################################
# get the pad and chunk lengths from the values in the ini file
paddata = int(cp.get('data', 'pad-data'))
n = int(cp.get('data', 'segment-length'))
s = int(cp.get('data', 'number-of-segments'))
r = int(cp.get('data', 'sample-rate'))
o = int(cp.get('inspiral', 'segment-overlap'))
length = ( n * s - ( s - 1 ) * o ) / r
overlap = o / r
minsciseg = length + 2 * paddata

##############################################################################
# Read in all the segment lists, filtering for sufficiently long segments
segdict = segments.segmentlistdict()

for ifo in ["G1", "H1", "H2", "L1", "V1"]:
  ifo_segfile = getattr(opts, "%s_segments" % ifo.lower())
  if ifo_segfile is not None:
    tmplist = segmentsUtils.fromsegwizard(open(ifo_segfile))
    segdict[ifo] = segments.segmentlist([s for s in tmplist if abs(s) > minsciseg])
ifolist = segdict.keys()
ifolist.sort()

##############################################################################
# Derive data quality vetoes
if opts.do_dataquality:
  dq_segdict = segments.segmentlistdict()
  
  for ifo in ifolist:
    # download (if necessary) and parse DQ segments
    dq_segfile = ifo + "_dq_segments.txt"
    dq_segdict.update(dqutils.download_and_parse_dq_segs(dq_segfile, ifo,
                                                         verbose=opts.verbose))
    
    # determine category 1 vetoes
    window_file = cp.get("segments", ifo.lower() + "-cat-1-veto-file")
    veto_windows = dqutils.from_veto_file(open(window_file))
    vetoed_segs = dqutils.determine_vetoes(veto_windows, dq_segdict,
                                           segdict[ifo])
    
    # apply veto
    segdict[ifo] -= vetoed_segs
    
    # write a copy for accounting purposes
    kept_filename = ifo + "_cat1_kept_segments.txt"
    vetoed_filename = ifo + "_cat1_vetoed_segments.txt"
    segmentsUtils.tosegwizard(open(kept_filename, "w"), segdict[ifo])
    segmentsUtils.tosegwizard(open(vetoed_filename, "w"), vetoed_segs)

############################################################################
# set up the Ã¼ber dag for all intervals and all injections

tag = opts.config_file.rstrip(".ini")
uberdag = pipeline.CondorDAG("%s/%s_uberdag.log" % (opts.log_path, tag))
uberdag.set_dag_file("%s_uberdag" % tag)

# hack to enable somewhat clean rescues
inspiralutils.write_rescue()

##############################################################################
# read in the GRB data from the list
ext_trigs = grbsummary.load_external_triggers(opts.list, verbose=opts.verbose)

if not (len(opts.grb) == 1 and opts.grb[0] == "all"):
  ext_trigs = ext_trigs.filterRows(lambda row: row.event_number_grb in opts.grb)

##############################################################################
# loop over the intervals, constructing overlapping segment lists,
# making directories, and writing output to them
grb_caches = []

for grb in ext_trigs:

  # extract the time of the GRB
  trigger = int(grb.start_time)

  ##############################################################################
  # set up the on source segment
  onSourceSegment = segments.segment(trigger - opts.onsource_left, trigger + opts.onsource_right)
  lengthOnsource = opts.onsource_left + opts.onsource_right

  # name and the directory
  idirectory = "GRB" + str(grb.event_number_grb)
  mkdir( idirectory, opts.overwrite_dir)

  # create the source-file which contains data about the position and time
  # and place it into the corresponding directory
  source_file = idirectory+"/triggerGRB"+str(grb.event_number_grb)+".xml"
  grbsummary.write_rows([grb], lsctables.ExtTriggersTable, source_file)

  ##############################################################################
  # set up the segment including the off-source segment

  offSourceSegment, grb_ifolist = \
    grbsummary.multi_ifo_compute_offsource_segment(segdict, onSourceSegment,
      padding_time=opts.padding_time, max_trials=opts.num_trials,
      symmetric=opts.symmetric)
  grb_ifolist.sort()
  ifo_times = "".join(grb_ifolist)
  
  if offSourceSegment is None:
    print "Warning: Trigger for GRB %s cannot be found in any segment for %s.  Skipping." % (grb.event_number_grb, ifo)
    continue
  elif opts.verbose:
    print "Sufficient off-source data has been found in", ifo_times, "time."
  
  if len(grb_ifolist) == 0:
    print >>sys.stderr, "Warning: could not construct offsource segment for GRB %s; skipping" % grb.event_number_grb
    continue
  if len(grb_ifolist) > 4:
    while len(grb_ifolist) > 4:
      del grb_ifolist[4]
    print >>sys.stderr, "Warning: inspiral_hipe cannot handle more than 4 IFOs; truncating to first four alphabetically."

  # write out the segment list to a segwizard file
  offsource_segfile = idirectory + "/offSourceSeg.txt"
  segmentsUtils.tosegwizard(open(offsource_segfile, "w"),
                            segments.segmentlist([offSourceSegment]))
  onsource_segfile = idirectory + "/onSourceSeg.txt"
  segmentsUtils.tosegwizard(file(onsource_segfile, "w"),
                            segments.segmentlist([onSourceSegment]))
  
  if opts.verbose:
    print "on-source segment: ", onSourceSegment
    print "off-source segment: ", offSourceSegment
  
  # plot the segment lists
  if opts.plot_segments or opts.plot_injections:
    plot_dir = "trigger_hipe_plots"
    inspiralutils.mkdir(plot_dir)
  if opts.plot_segments:
    plot_segments(trigger, grb_ifolist, segdict, onSourceSegment, offSourceSegment,
      plot_dir)

  # Next thing is to generate the dag for this interval of time.
  # The steps here are:
  #   1. make dir for zero-lag and playground
  #   2. copy in ini file (and modify if needed)
  #   3. generate dag
  #   4. make dir for injections and run inspinj
  #   5. repeat 2 & 3
  #   6. repeat 4 & 5 as needed

  ############################################################################
  # set up the analysis dag for this interval
  #
  # In doing this, we simply copy the configuration file into the
  # sub-directory and then run the dag generation script.
  
  hipe_caches = []
  
  if opts.verbose: print "Creating datafind DAG"
  datafind_dag = hipe_run(idirectory + "/datafind", cp, grb_ifolist,
                          opts.log_path, source_file, "datafind",
                          offSourceSegment, usertag=idirectory + "_DATAFIND",
                          verbose=opts.verbose, only_run=["datafind"])
  datafind_dag.write_dag()
  datafind_node = datafind_dag.get_dag_node()
  uberdag.add_node(datafind_node)
  
  if opts.verbose: print "Creating DAG for offsource analysis"
  offsource_analysis = hipe_run(idirectory + "/offsource", cp, grb_ifolist,
                            opts.log_path, source_file, "offsource",
                            offSourceSegment,
                            usertag=idirectory + "_OFFSOURCE",
                            exttrig_analyze="off_source", verbose=opts.verbose,
                            dont_run=["datafind"])
  offsource_analysis.write_dag()
  offsource_analysis_node = offsource_analysis.get_dag_node()
  offsource_analysis_node.add_parent(datafind_node)
  uberdag.add_node(offsource_analysis_node)
  hipe_caches.append(offsource_analysis.get_cache_name())
  
  if opts.verbose: print "Creating DAG for onsource analysis"
  onsource_analysis = hipe_run(idirectory + "/onsource", cp, grb_ifolist,
                               opts.log_path, source_file, "onsource",
                               onSourceSegment, exttrig_analyze="on_source",
                               usertag=idirectory + "_ONSOURCE",
                               verbose=opts.verbose, dont_run=["datafind"])
  onsource_analysis.write_dag()
  onsource_analysis_node = onsource_analysis.get_dag_node()
  onsource_analysis_node.add_parent(datafind_node)
  uberdag.add_node(onsource_analysis_node)
  hipe_caches.append(onsource_analysis.get_cache_name())

  ############################################################################
  # create the config parser object and read in the ini file
  if opts.injection_config:
    cpinj = ConfigParser.ConfigParser()
    cpinj.read(opts.injection_config)
  
    ############################################################################
    # set up the injection dag for this interval
    for injrun in cpinj.sections():
      if opts.verbose: print "Creating DAG for", injrun
      hipe_dir = os.path.join(idirectory, injrun)
      usertag = idirectory + "_" + injrun
      injection_analysis = hipe_run(hipe_dir, cp, grb_ifolist,
                                    opts.log_path, source_file, injrun,
                                    offSourceSegment, usertag=usertag,
                                    verbose=opts.verbose,
                                    exttrig_analyze="off_source",
                                    dont_run=["datafind"])

      # create the master injection file and gets its content
      if opts.verbose: print "  Creating injections..."
      sims, injInterval, numberInjections = createInjectionFile(hipe_dir, cp,
        cpinj, injrun, offSourceSegment.contract(opts.padding_time),
        source_file, opts.verbose)
        
      # XXX: hack since glue is having trouble with empty columns
      fix_numrel_columns(sims)

      # plot the injections as made
      if opts.plot_injections:
          if opts.verbose: print "  Plotting injection distributions..."
          plot_injection_distributions(sims, injrun, plot_dir,
            overwrite_flag=opts.overwrite_dir)

      # split the injections
      if opts.verbose: print "  Splitting injections..."
      safetyInterval = 800 # safety time in seconds between two injections
      deltaIndex = int(safetyInterval // injInterval) + 1
      for i in range(deltaIndex):
          inj_file_name = "%s/HL-INJECTION_%s_%d-%d-%d.xml" % \
              (hipe_dir, usertag, i+1, offSourceSegment[0],
               abs(offSourceSegment))
          sims_chunk = sims[i:numberInjections:deltaIndex]
          grbsummary.write_rows(sims_chunk, lsctables.SimInspiralTable, inj_file_name)

      # set the needed parameters
      if opts.verbose: print "  Writing DAG..."
      injection_analysis.set_numslides(0)
      injection_analysis.set_injections(injrun, deltaIndex)
      injection_analysis.write_dag()
      injection_analysis_node = injection_analysis.get_dag_node()
      injection_analysis_node.add_parent(datafind_node)
      uberdag.add_node(injection_analysis_node)
      hipe_caches.append(injection_analysis.get_cache_name())
  
  grb_cache_name = idirectory + "/" + idirectory + ".cache"
  concatenate_cache_files(hipe_caches, grb_cache_name)
  grb_caches.append(grb_cache_name)

if opts.verbose: print "Writing sub files and uber-DAG..."
uberdag.write_sub_files()
uberdag.write_dag()

concatenate_cache_files(grb_caches, tag + ".cache")

print """
If you run the uber-dag, you probably have to tell Condor not to
complain that your DAG logs are on NFS volumes:

bash users:
export _CONDOR_DAGMAN_LOG_ON_NFS_IS_ERROR=FALSE

tcsh users:
setenv _CONDOR_DAGMAN_LOG_ON_NFS_IS_ERROR FALSE
"""
