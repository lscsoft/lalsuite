#!/usr/bin/env @PYTHONPROG@
"""
ihope.in - weekly automate pipeline driver script

This script generates the condor DAG necessary to analyze a given amount
of LIGO and GEO data.  It takes in the start and end times, generates the
segment lists, runs the zero-lag, time shift and injection analyses, generates
summary information and plots and follows up the loudest events.
"""
__author__ = 'Stephen Fairhurst <sfairhur@gravity.phys.uwm.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import os, sys, copy, shutil, re
import ConfigParser
import optparse
import tempfile
import urllib
import time
import calendar
import shutil
import M2Crypto

sys.path.append('@PYTHONLIBDIR@')

##############################################################################
# import the modules we need to build the pipeline
from glue import segments
from glue import segmentsUtils
from glue import pipeline
import inspiralutils 


##############################################################################
#
#  MAIN PROGRAM
#
##############################################################################
usage = """usage: %prog [options] 

lalapps_ihope is designed to run the inspiral analysis end to end.  It
performs several distinct steps.  The default is for the full search to
be run, various steps can be skipped by specifying --skip options.  The
required arguments are:

--config-file 
--log path
--gps-start-time
--gps-end-time

The options to run the analysis are specified in the config-file
(normally ihope.ini) file.

1) Generate the segments for which the analysis should be done.  This
can be skipped using --skip-generate-segments.  The option
--use-available-data will restrict the search to those times for which
there is data on the cluster the analysis is being run.

2) Generate the veto segments.  Can be skipped with
--skip-generate-veto-segments.

3) Find the data on the cluster.  Can be skipped with --skip-datafind.

4) Generate the template banks.  Can be skipped with --skip-tmpltbank.

5) The search itself.  Can be skipped with --skip-search.

6) The application of the data quality vetoes.  Can be skipped with
--skip-data-quality.

7) Make plots of the output.  Can be skipped with --skip-plots.

8) Followup the loudest events (currently not working).  Can be skipped
with --skip-followup.

For options 5-8, you can run 3 types of analysis:
a) playground and playground time slides. Skipped with --skip-playground
b) full data and full data time slides.  Skipped with --skip-full-data
c) software injections. Skipped with --skip-injections
"""

parser = optparse.OptionParser( usage=usage, \
      version= "%prog CVS\n" +
      "$Id$\n" +
      "$Name$\n")

# arguments
parser.add_option("-f", "--config-file",action="store",type="string",\
    metavar=" FILE", help="use configuration file FILE")

parser.add_option("-p", "--log-path",action="store",type="string",\
    metavar=" PATH", \
    help="directory to write condor log file, should be a local directory")

parser.add_option("-l", "--node-local-dir",action="store",type="string",\
    metavar=" NODEPATH", \
    help='''directory to write the temporary sql databases. This must be a
directory that is local to all nodes.''')

parser.add_option("-o", "--node-access-path",action="store",type="string",\
    metavar=" OUTPATH", \
    help="node path for outputting the temporary coherent files to.")

parser.add_option("-u", "--username",action="store",type="string",\
    metavar=" USERNAME", help='''username on cluster for constructing the
full path-name for outputting the temporary coherent files.''')

parser.add_option("-s", "--gps-start-time",action="store",type="int",\
    metavar=" GPS_START", help="begin analysis at GPS_START")

parser.add_option("-e", "--gps-end-time",action="store",type="int",\
    metavar=" GPS_END", help="end analysis at GPS_END")

parser.add_option("-D", "--use-available-data",action="store_true",\
    default=False, help="analyze only the data available on the cluster")

parser.add_option("-R", "--reverse-analysis",action="store_true",\
    default=False, help="do the entrire analysis using reverse chirp method")

parser.add_option("-S", "--skip-generate-segments",action="store_false",\
    default=True, dest="generate_segments", \
    help="skip generation segments for analysis")

parser.add_option("-V", "--skip-generate-veto-segments",action="store_false",\
    default=True, dest="generate_veto_segments", \
    help="skip generation segments for analysis")

parser.add_option("-B", "--skip-datafind",action="store_false",\
    default=True, dest="run_datafind", help="skip the datafind step")

parser.add_option("-T", "--skip-tmpltbank",action="store_false",\
    default=True, dest="run_tmpltbank", help="skip the template bank generation")

parser.add_option("-F", "--skip-full-data",action="store_false",\
    default=True, dest="run_full_data", help="skip the full data + slides")

parser.add_option("-P", "--skip-playground",action="store_false",\
    default=True, dest="run_playground", help="skip the playground analysis")

parser.add_option("-I", "--skip-injections",action="store_false",\
    default=True, dest="run_injections", \
    help="skip the inspiral analysis with software injections")

parser.add_option("-A", "--skip-search",action="store_false",\
    default=True, dest="run_search",
    help="skip the search of the data (i.e. don't run inspiral hipe)")

parser.add_option("-Q", "--skip-data-quality",action="store_false",\
    default=True, dest="run_data_quality", \
    help="skip generation dq veto segments and use in analysis ")

parser.add_option("-Z", "--skip-plots",action="store_false",\
    default=True, dest="run_plots",  help="skip the plotting step")

parser.add_option("-U", "--skip-followup",action="store_false",\
    default=True, dest="run_followup",  help="skip the inspiral followup")

parser.add_option("-H", "--skip-pipedown",action="store_false",\
    default=True, dest="run_pipedown",  help="skip running pipedown")

parser.add_option("-W","--skip-hardware-injections",action="store_false",\
    default=True,dest="run_hardware_inj", help="Skip the hardware injection script")

parser.add_option("-w","--skip-omega-scans",action="store_false",\
    default=True,dest="do_omega_scans", help="Skip the omega scan setup.")

parser.add_option("-x", "--dax",action="store_true",\
    default=False, help="Delete the ligo_data_find jobs and populate frame LFNs in the DAX")

parser.add_option("-g", "--grid-site",action="store",type="string",\
    metavar="SITE", help="Specify remote site in conjunction with --dax option")

parser.add_option("--data-checkpoint", action="store_true",default=False,\
    help="checkpoint the inspiral code")

parser.add_option("--use-gpus", action="store_true", default=False,\
    help="run inspiral jobs on GPU nodes")

command_line = sys.argv[1:]
(opts,args) = parser.parse_args()

##############################################################################
# Sanity check of input arguments
if not opts.config_file:
  print >> sys.stderr, "No configuration file specified."
  print >> sys.stderr, "Use --config-file FILE to specify location."
  sys.exit(1)

if not opts.config_file[-4:]==".ini":
  print >> sys.stderr, "Configuration file name must end in '.ini'!" 
  sys.exit(1)

if not opts.log_path:
  print >> sys.stderr, "No log file path specified."
  print >> sys.stderr, "Use --log-path PATH to specify a location."
  sys.exit(1)

if opts.run_pipedown:
  if not opts.node_local_dir:
    print >> sys.stderr, "No local dir specified. If running with pipedown"
    print >> sys.stderr, "use --node-local-dir to specify a local directory"
    print >> sys.stderr, "Use --help for more information"
    sys.exit(1)

if not opts.gps_start_time:
  print >> sys.stderr, "No GPS start time specified for the analysis"
  print >> sys.stderr, "Use --gps-start-time GPS_START to specify a location."
  sys.exit(1)

if not opts.gps_end_time:
  print >> sys.stderr, "No GPS end time specified for the analysis"
  print >> sys.stderr, "Use --gps-end-time GPS_END to specify a location."
  sys.exit(1)

if opts.gps_end_time < opts.gps_start_time:
  print >> sys.stderr, "The GPS end time must be after the GPS start time"
  sys.exit(1)

if not opts.node_access_path:
  print >> sys.stdout, "WARNING! No node-access-path specified:"
  print >> sys.stdout, "  Use --node-access-path and --username to speed up"
  print >> sys.stdout, "  the coherent-stage."

if not opts.username:
  print >> sys.stdout, "WARNING! No username specified:"
  print >> sys.stdout, "  Use --node-access-path and --username to speed up"
  print >> sys.stdout, "  the coherent-stage."

opts.complete_cache = (opts.run_data_quality or opts.run_plots or opts.run_pipedown or opts.run_search)

##############################################################################
# check the user has a valid grid proxy

def check_grid_proxy(path):
  try:
    proxy = M2Crypto.X509.load_cert(path)
  except Exception, e:
    msg = "Unable to load proxy from path %s : %s" % (path, e)
    raise RuntimeError, msg

  try:
    proxy.get_ext("proxyCertInfo")
  except LookupError:
    subject = proxy.get_subject().as_text()
    if re.search(r'.+CN=proxy$', subject):
      msg = "Proxy %s is not RFC compliant" % path
      raise RuntimeError, msg

  try:
    expireASN1 = proxy.get_not_after().__str__()
    expireGMT  = time.strptime(expireASN1, "%b %d %H:%M:%S %Y %Z")
    expireUTC  = calendar.timegm(expireGMT)
    now = int(time.time())
  except Exception, e:
    msg = "could not determine time left on proxy: %s" % e
    raise RuntimeError, msg

  return expireUTC - now

time_needed = 3600 * 6
tmp_proxy_time_left = proxy_time_left = 0
my_uid = os.getuid()
proxy_path = "/tmp/x509up_u%d" % my_uid 

try:
  # check to see how much time is left on the environment's proxy
  if os.environ.has_key('X509_USER_PROXY'):
    tmp_proxy_path = os.environ['X509_USER_PROXY']
    if os.access(tmp_proxy_path, os.R_OK):
      try:
        tmp_proxy_time_left = check_grid_proxy(tmp_proxy_path)
      except:
        tmp_proxy_time_left = 0

  # check to see how much time there is left on the proxy in /tmp
  if os.access(proxy_path, os.R_OK):
    try:
      proxy_time_left = check_grid_proxy(proxy_path)
    except:
      proxy_time_left = 0

  # use the one with the most time left
  if tmp_proxy_time_left > 0 and tmp_proxy_time_left > proxy_time_left:
    shutil.copy(tmp_proxy_path,proxy_path)

  # check that the proxy is valid and that enough time remains
  time_left = check_grid_proxy(proxy_path)
  if time_left < 0:
    raise RuntimeError, "Proxy has expired."
  elif time_left < time_needed:
    msg = "Not enough time left on grid proxy (%d seconds)." % time_left
    raise RuntimeError, msg
  else:
    os.environ['X509_USER_PROXY'] = proxy_path

except:
  print >> sys.stderr, """
Error: Could not find a valid grid proxy. Please run 

   ligo-proxy-init albert.einstein

with your LIGO.ORG username to generate a new grid proxy certificate.

You may need to log out and do this on your home machine if your .globus
directory is not present on the machine where you are running ihope.

You can check the status of your grid proxy by running

  grid-proxy-info

At least %d seconds must be left before your proxy expires to run ihope.
""" % time_needed
  raise

##############################################################################
# set up the analysis directory
analysisDirectory = str(opts.gps_start_time) + "-" + str(opts.gps_end_time)
inspiralutils.mkdir(analysisDirectory)

# copy the ini file into the directory
shutil.copy( opts.config_file, analysisDirectory )
opts.config_file = opts.config_file.split("/")[-1]

os.chdir(analysisDirectory)
inspiralutils.mkdir('logs')

# parse the ini file:
cp = ConfigParser.ConfigParser()
cp.read(opts.config_file)

# set gps start and end times in the cp object.
cp.set("input", "gps-start-time", str(opts.gps_start_time) )
cp.set("input", "gps-end-time", str(opts.gps_end_time) )
cp.set("input", "output-path", str(opts.node_access_path) )
cp.set("input", "username", str(opts.username) )

# Check for --disable-dag-categories

opts.do_dag_categories = not cp.has_option(
              "hipe-arguments","disable-dag-categories")

# Tell matplotlib to put its cache files in the the local dir
cp.set("pipeline", "matplotlibdir", opts.node_local_dir )

# Add local dir to pipeline
if opts.run_pipedown:
  cp.set("pipeline", "node-tmp-dir", opts.node_local_dir)

# Add GPU flag
if opts.use_gpus:
  cp.set('condor', 'use-gpus', '')

##############################################################################
# create a directory called executables and copy them over
inspiralutils.mkdir("executables")
os.chdir("../")
for (job, executable) in cp.items("condor"):
  if job not in ("universe", "use-gpus") and executable != "":
    shutil.copy( executable, analysisDirectory + "/executables" )
    if job == "hardware_inj_page":
      executable = "executables/" + executable.split("/")[-1]
    else:
      executable = "../executables/" + executable.split("/")[-1]
    cp.set("condor", job, executable)

os.chdir(analysisDirectory)

##############################################################################
# create a log file that the Condor jobs will write to
# 
# we already checked that the last 4 characters of the config file are '.ini'
basename = opts.config_file[:-4]
logname = basename + '.dag.log.'
try:
  os.mkdir(opts.log_path)
except:
  pass
tempfile.tempdir = opts.log_path
logfile = tempfile.mktemp(prefix=logname)
fh = open( logfile, "w" )
fh.close()

##############################################################################
# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile,dax=opts.dax)
dag.set_dag_file(basename)
dag.set_dax_file(basename)

##############################################################################
# Set up the IFOs and get the appropriate segments
ifos = [] 

for option in ["g1-data","h1-data","h2-data","l1-data","v1-data"]:
  if cp.has_option("ifo-details",option): ifos.append(option[0:2].upper() )

if cp.has_option("ifo-details","analyze-all"): 
  print >> sys.stderr, \
      "The inspiral pipeline does not yet support coincidence between"
  print >> sys.stderr, "all five IFOs. Do not use the analyze-all option."
  sys.exit(1)

ifos.sort()

print "Setting up an analysis for " + str(ifos) + " from " + \
    str(opts.gps_start_time) + " to "  + str(opts.gps_end_time)
print
sys.stdout.flush()

##############################################################################
# create a local execute directory for the pipeline
tmp_exec_dir = tempfile.mkdtemp( prefix="%s-%s-%s-%d." % (''.join(ifos),
  basename, opts.gps_start_time, 
  int(opts.gps_end_time) - int(opts.gps_start_time)) )
os.chmod(tmp_exec_dir, 0755)

##############################################################################
# Determine the segments to analyze

# first, copy the segment files and make sure that the ini file reflects the
# new path, relative to the DAG directories.

inspiralutils.mkdir("segments")
os.chdir("..")
for vetoes, infile in cp.items("segments"):
  inspiralutils.copyCategoryFiles(cp,vetoes,"segments",infile,\
                analysisDirectory)

os.chdir(analysisDirectory)
os.chdir("segments")

# Determine which veto categories to filter
veto_categories = [int(cat) for cat in cp.get('segments','veto-categories').split(',')] 
veto_categories.sort()

# Download the veto definer xml file
vetoDefFile = inspiralutils.downloadVetoDefFile(cp, opts.generate_segments)

# Generate veto xml files for each ifo and for each category
inspiralutils.generate_veto_cat_files(cp, vetoDefFile, \
    opts.generate_veto_segments)

# Generate a combined veto file for each ifo for use by pipedown
gps_start = int(opts.gps_start_time)
gps_end = int(opts.gps_end_time)
duration = gps_end - gps_start

dqVetoes = {}
dqVetoes["combined-veto-file"] = {}
for category in [1] + veto_categories:
  ifos_string = ''
  veto_cat_files = []
  veto_cat_filename = str(gps_start) + '-' + str(duration) + '.xml'
  for ifo in ifos:
    veto_cat_files.append('-'.join([ifo, 'VETOTIME_CAT' + str(category), veto_cat_filename]))
    ifos_string += ifo
  dqVetoes["combined-veto-file"][category] = '-'.join([ifos_string, 'VETOTIME_CAT_' + str(category), veto_cat_filename])
  concat_veto_call = ' '.join([ cp.get("condor", "ligolw_add"),
    "--output", dqVetoes["combined-veto-file"][category]] + veto_cat_files )
  inspiralutils.make_external_call(concat_veto_call)

  # FIXME: remove this block  when the veto files from
  # ligolw_segments_from_cats have contain a segment table with start_time_ns
  # and end_time_ns, and when the process table doesn't need the domain, jobid,
  # and is_online columns added

  compat_veto_call = ' '.join([cp.get("condor", "ligolw_segments_compat"), dqVetoes["combined-veto-file"][category]])
  inspiralutils.make_external_call(compat_veto_call)

segFile = {}

for ifo in ifos:
  segFile[ifo], dqVetoes[ifo] = inspiralutils.findSegmentsToAnalyze(
      cp, ifo, veto_categories,
      opts.generate_segments, opts.use_available_data,
      opts.generate_veto_segments)
  # set correct name as we're in the segments directory:
  segFile[ifo] = "../segments/" + segFile[ifo]
  cp.set("input", ifo.lower() + "-segments", segFile[ifo])
  for key in dqVetoes[ifo].keys():
    dqVetoes[ifo][key] = "../segments/" + dqVetoes[ifo][key]
for key in dqVetoes["combined-veto-file"].keys():
  dqVetoes["combined-veto-file"][key] = "../segments/" + dqVetoes["combined-veto-file"][key]

os.chdir("..")

hw_inj_dir = "hardware_injection_summary"
if opts.run_hardware_inj:
  inspiralutils.mkdir(hw_inj_dir)
  inspiralutils.get_hwinj_segments(cp,ifos,hw_inj_dir)

if opts.do_omega_scans:
  if not os.path.isdir('omega_setup'):
    os.mkdir('omega_setup')
  os.chdir('omega_setup')
  inspiralutils.omega_scan_setup(cp,ifos)
  os.chdir('..')

# create the list of files that will be concatenated into the ihope cache
cachelist = []
cachefilename = basename + '.cache'

##############################################################################
# Run lalapps_inspiral_hipe for datafind and template bank generation
if opts.run_datafind or opts.run_tmpltbank:
  print "Running inspiral hipe for datafind/template bank run"
  hipeDfNode = inspiralutils.hipe_setup("datafind", cp, ifos, \
      opts.log_path, dataFind = opts.run_datafind, tmpltBank = opts.run_tmpltbank,
      dax=opts.dax, local_exec_dir=tmp_exec_dir)
  for cacheFile in hipeDfNode.get_output_files():
      cachelist.append("datafind/" + cacheFile)
  dag.add_node(hipeDfNode)

##############################################################################
# Set up the directories for each run and run lalapps_inspiral_hipe

if opts.complete_cache:
  # We need to get the names of the template bank files that are generated in
  # the datafind step, so we can link them into all the search directories
  datafind_cache_filename = "datafind/" + inspiralutils.hipe_cache(ifos, \
      None, opts.gps_start_time, opts.gps_end_time)
  tmpltbank_cache = inspiralutils.tmpltbank_cache(datafind_cache_filename)

  if opts.run_playground:
    playDir = "playground"
    if opts.run_search:
      print "Running inspiral hipe for playground run"
      hipePlayNode = inspiralutils.hipe_setup(playDir, cp, ifos, \
           opts.log_path, playOnly = True, dax=opts.dax, \
           tmpltbankCache = tmpltbank_cache, local_exec_dir=tmp_exec_dir)
      for cacheFile in hipePlayNode.get_output_files():
        cachelist.append(playDir + "/" + cacheFile)
      dag.add_node(hipePlayNode)
      if opts.run_datafind or opts.run_tmpltbank:
        hipePlayNode.add_parent(hipeDfNode)
    else:
      if cp.get("pipeline","user-tag"):
        usertag = ecp.get("pipeline", "user-tag") + "_" + playDir.upper()
      else:
        usertag = playDir.upper()
      cacheFile = inspiralutils.hipe_cache( ifos,usertag, \
          cp.getint("input", "gps-start-time"), \
          cp.getint("input", "gps-end-time")) 
      if os.path.isfile(playDir + "/" + cacheFile):
        cachelist.append(playDir + "/" + cacheFile)
      else:
        print>>sys.stderr, "WARNING: Cache file " + playDir + "/" + cacheFile
        print>>sys.stderr, "does not exist! This might cause later failures."


  if opts.run_full_data:
    fullDir = "full_data"
    if opts.run_search:
      print "Running inspiral hipe for analysis run"
      hipeAnalysisNode = inspiralutils.hipe_setup(fullDir, cp, ifos, \
          opts.log_path,dax=opts.dax, \
          tmpltbankCache = tmpltbank_cache, local_exec_dir=tmp_exec_dir,
          data_checkpoint = opts.data_checkpoint)
      for cacheFile in hipeAnalysisNode.get_output_files():
        cachelist.append(fullDir + "/" + cacheFile)
      dag.add_node(hipeAnalysisNode)
      if opts.run_datafind or opts.run_tmpltbank:
        hipeAnalysisNode.add_parent(hipeDfNode)
    else:
      if cp.get("pipeline","user-tag"):
        usertag = cp.get("pipeline", "user-tag") + "_" + fullDir.upper()
      else:
        usertag = fullDir.upper()
      cacheFile = inspiralutils.hipe_cache( ifos,usertag, \
          cp.getint("input", "gps-start-time"), \
          cp.getint("input", "gps-end-time")) 
      if os.path.isfile(fullDir + "/" + cacheFile):
        cachelist.append(fullDir + "/" + cacheFile)
      else:
        print>>sys.stderr, "WARNING: Cache file " + fullDir + "/" + cacheFile
        print>>sys.stderr, "does not exist! This might cause later failures."

  
  if opts.run_injections:
    hipeInjNode = {} 
    print "Running inspiral hipe for injection runs"
    for (injDir, injSeed) in cp.items("injections"):
      if opts.run_search:
        hipeInjNode[injDir] = inspiralutils.hipe_setup(injDir, cp, ifos, \
             opts.log_path, injSeed, dax=opts.dax, \
             tmpltbankCache = tmpltbank_cache, local_exec_dir=tmp_exec_dir,
             data_checkpoint = opts.data_checkpoint)
        for cacheFile in hipeInjNode[injDir].get_output_files():
          cachelist.append(injDir + "/" + cacheFile)
        dag.add_node(hipeInjNode[injDir])
        if opts.run_datafind or opts.run_tmpltbank:
          hipeInjNode[injDir].add_parent(hipeDfNode)
      else:
        if cp.get("pipeline","user-tag"):
          usertag = cp.get("pipeline", "user-tag") + "_" + injDir.upper()
        else:
          usertag = injDir.upper()
        cacheFile = inspiralutils.hipe_cache( ifos,usertag, \
            cp.getint("input", "gps-start-time"), \
            cp.getint("input", "gps-end-time")) 
        if os.path.isfile(injDir + "/" + cacheFile):
          cachelist.append(injDir + "/" + cacheFile)
        else:
          print>>sys.stderr, "WARNING: Cache file " + injDir + "/" + cacheFile
          print>>sys.stderr, "does not exist! This might cause later failures."


################################################################################
# Setting up directories for reverse analysis

if opts.reverse_analysis:

  if opts.run_play_analysis:
    cp.set ("inspiral","reverse-chirp-bank","")
    print "Running inspiral hipe for reverse playground run"
    hipePlayNode = inspiralutils.hipe_setup("playground_reverse", cp, ifos, \
         opts.log_path, playOnly = True, dax = opts.dax, \
         tmpltbankCache = tmpltbank_cache, local_exec_dir=tmp_exec_dir )
    for cacheFile in hipePlayNode.get_output_files():
      cachelist.append("playground_reverse/" + cacheFile)
    dag.add_node(hipePlayNode)
    if opts.run_datafind or opts.run_tmpltbank:
      hipePlayNode.add_parent(hipeDfNode)

  if opts.run_full_data:
    cp.set ("inspiral","reverse-chirp-bank","")
    print "Running inspiral hipe for reverse analysis run"
    hipeAnalysisNode = inspiralutils.hipe_setup("full_data_reverse", cp, ifos, \
        opts.log_path, dax=opts.dax, \
        tmpltbankCache = tmpltbank_cache, local_exec_dir=tmp_exec_dir )
    for cacheFile in hipeAnalysisNode.get_output_files():
      cachelist.append("full_data_reverse/" + cacheFile)
    dag.add_node(hipeAnalysisNode)
    if opts.run_datafind or opts.run_tmpltbank:
      hipeAnalysisNode.add_parent(hipeDfNode)

  if opts.run_injections:
    hipeInjNode = {}
    cp.set ("inspiral","reverse-chirp-bank","")
    print "Running inspiral hipe for reverse injection runs"
    for (injDir, injSeed) in cp.items("injections_reverse"):
      hipeInjNode[injDir] = inspiralutils.hipe_setup(injDir, cp, ifos, \
           opts.log_path, injSeed, dax=opts.dax, \
           tmpltbankCache = tmpltbank_cache, local_exec_dir=tmp_exec_dir )
      for cacheFile in hipeInjNode[injDir].get_output_files():
        cachelist.append(injDir + "/" + cacheFile)
      dag.add_node(hipeInjNode[injDir])
      if opts.run_datafind or opts.run_tmpltbank:
        hipeInjNode[injDir].add_parent(hipeDfNode)

##############################################################################
# Run the data quality vetoes
if opts.complete_cache:
  hipePlayVetoNode = {}
  hipeAnalysisVetoNode = {}
  hipeInjVetoNode = {}

  for category in veto_categories:
    hipeInjVetoNode[category] = {}
    if opts.run_data_quality:
      print "Setting up the category " + str(category) + " veto dags"

    if opts.run_playground:
      playDir = "playground"
      if opts.run_data_quality:
        print "Running inspiral hipe for play with vetoes"
        hipePlayVetoNode[category] = inspiralutils.hipe_setup(playDir, cp, \
            ifos, opts.log_path, playOnly = True, vetoCat = category, \
            vetoFiles = dqVetoes, dax=opts.dax, local_exec_dir=tmp_exec_dir)
        for cacheFile in hipePlayVetoNode[category].get_output_files():
          cachelist.append(playDir + "/" + cacheFile)
        dag.add_node(hipePlayVetoNode[category])
        if category >= 2 and opts.run_search:
          hipePlayVetoNode[category].add_parent(hipePlayNode)
      elif not opts.run_search:
        if cp.get("pipeline","user-tag"):
          usertag = cp.get("pipeline", "user-tag") + "_" + playDir.upper()
        else:
          usertag = playDir.upper() 
        usertag += "_CAT_" + str(category) + "_VETO"
        cacheFile = inspiralutils.hipe_cache( ifos,usertag, \
            cp.getint("input", "gps-start-time"), \
            cp.getint("input", "gps-end-time"))  
        if os.path.isfile(playDir + "/" + cacheFile):
          cachelist.append(playDir + "/" + cacheFile)
        else:
          print>>sys.stderr, "WARNING: Cache file " + playDir + "/" + cacheFile
          print>>sys.stderr, "does not exist! This might cause later failures."



    if opts.run_full_data:
      fullDir = "full_data"
      if opts.run_data_quality:
        print "Running inspiral hipe for full data with vetoes"
        hipeAnalysisVetoNode[category] = inspiralutils.hipe_setup(fullDir, \
            cp, ifos, opts.log_path, vetoCat = category, vetoFiles = dqVetoes, \
            dax = opts.dax, local_exec_dir=tmp_exec_dir,
            data_checkpoint = opts.data_checkpoint)
        for cacheFile in hipeAnalysisVetoNode[category].get_output_files():
          cachelist.append(fullDir + "/" + cacheFile)
        dag.add_node(hipeAnalysisVetoNode[category])
        if category >= 2 and opts.run_search:
          hipeAnalysisVetoNode[category].add_parent(hipeAnalysisNode)
      elif not opts.run_search:
        if cp.get("pipeline","user-tag"):
          usertag = cp.get("pipeline", "user-tag") + "_" + fullDir.upper()
        else:
          usertag = fullDir.upper()
        usertag += "_CAT_" + str(category) + "_VETO"
        cacheFile = inspiralutils.hipe_cache( ifos,usertag, \
            cp.getint("input", "gps-start-time"), \
            cp.getint("input", "gps-end-time")) 
        if os.path.isfile(fullDir + "/" + cacheFile):
          cachelist.append(fullDir + "/" + cacheFile)
        else:
          print>>sys.stderr, "WARNING: Cache file " + fullDir + "/" + cacheFile
          print>>sys.stderr, "does not exist! This might cause later failures."


			      
    if opts.run_injections:
      if opts.run_data_quality:
        print "Running inspiral hipe for injections with vetoes"
      for (injDir, injSeed) in cp.items("injections"):
        if opts.run_data_quality:
          hipeInjVetoNode[category][injDir] = inspiralutils.hipe_setup(injDir, \
              cp, ifos, opts.log_path, \
              injSeed= injSeed, vetoCat = category, vetoFiles = dqVetoes, \
              dax=opts.dax, local_exec_dir=tmp_exec_dir,
              data_checkpoint = opts.data_checkpoint)
          for cacheFile in hipeInjVetoNode[category][injDir].get_output_files():
            cachelist.append(injDir + "/" + cacheFile)
          dag.add_node(hipeInjVetoNode[category][injDir])
          if category >= 2 and opts.run_search:
            hipeInjVetoNode[category][injDir].add_parent(hipeInjNode[injDir])
        elif not opts.run_search:
          if cp.get("pipeline","user-tag"):
            usertag = cp.get("pipeline", "user-tag") + "_" + injDir.upper()
          else:
            usertag = injDir.upper()
          usertag += "_CAT_" + str(category) + "_VETO"
          cacheFile = inspiralutils.hipe_cache( ifos,usertag, \
              cp.getint("input", "gps-start-time"), \
              cp.getint("input", "gps-end-time")) 
          if os.path.isfile(injDir + "/" + cacheFile):
            cachelist.append(injDir + "/" + cacheFile)
          else:
            print>>sys.stderr, "WARNING: Cache file " + injDir + "/" + cacheFile
            print>>sys.stderr, "does not exist! This might cause later failures."


#############################################################################
# run the data quality vetoes for reverse chirp analysis

if opts.reverse_analysis:
  if opts.run_data_quality:
    hipePlayVetoNode = {}
    hipeAnalysisVetoNode = {}
    hipeInjVetoNode = {}

    for category in veto_categories:
      print "Setting up the category " + str(category) + " veto dags" + " for reverse chirp analysis"
      cp.set ("inspiral","reverse-chirp-bank","")
      if opts.run_play_analysis:
        print "Running inspiral hipe for playground  with vetoes for reverse analysis"

        hipePlayVetoNode[category] = inspiralutils.hipe_setup("playground_reverse", cp, \
            ifos, opts.log_path, playOnly = True, vetoCat = category, \
            vetoFiles = dqVetoes, local_exec_dir=tmp_exec_dir)
        for cacheFile in hipePlayVetoNode[category].get_output_files():
          cachelist.append("playground-reverse/" + cacheFile)
        dag.add_node(hipePlayVetoNode[category])
        if category==2 and opts.run_search:
          hipePlayVetoNode[category].add_parent(hipePlayNode)
        elif category>2:
          hipePlayVetoNode[category].add_parent(hipePlayVetoNode[category-1])

      if opts.run_full_data:
        print "Running inspiral hipe for full data with vetoes for reverse analysis"
        hipeAnalysisVetoNode[category] = inspiralutils.hipe_setup("full_data_reverse", \
            cp, ifos, opts.log_path, vetoCat = category, vetoFiles = dqVetoes, \
            local_exec_dir=tmp_exec_dir)
        for cacheFile in hipeAnalysisVetoNode[category].get_output_files():
          cachelist.append("full_data_reverse/" + cacheFile)
        dag.add_node(hipeAnalysisVetoNode[category])
        if category==2 and opts.run_search:
          hipeAnalysisVetoNode[category].add_parent(hipeAnalysisNode)
        elif category>2:
          hipeAnalysisVetoNode[category].add_parent( \
              hipeAnalysisVetoNode[category-1])

      if opts.run_injections:
        print "Running inspiral hipe for injections with vetoes for reverse analysis"
        for (injDir, injSeed) in cp.items("injections-reverse"):
          if category == 2: hipeInjVetoNode[injDir] = {}
          hipeInjVetoNode[injDir][category] = inspiralutils.hipe_setup(injDir, \
              cp, ifos, opts.log_path, \
              injSeed= injSeed, vetoCat = category, vetoFiles = dqVetoes, \
              local_exec_dir=tmp_exec_dir)
          for cacheFile in hipeInjVetoNode[injDir][category].get_output_files():
            cachelist.append(injDir + "/" + cacheFile)
          dag.add_node(hipeInjVetoNode[injDir][category])
          if category==2 and opts.run_search:
            hipeInjVetoNode[injDir][category].add_parent(hipeInjNode[injDir])
          elif category>2:
            hipeInjVetoNode[injDir][category].add_parent(\
                hipeInjVetoNode[injDir][category-1])

##############################################################################
# cat the cache files together
  

if len(cachelist):
  command = "cat " + " ".join(cachelist) + " > " + cachefilename
  os.popen(command)
  
##############################################################################
# Run lalapps_pipedown

if opts.run_pipedown:
  # Set up parents correctly
  parentNodes = []
  if opts.run_search:
    if opts.run_full_data:
      parentNodes.append(hipeAnalysisNode)
    if opts.run_playground:
      parentNodes.append(hipePlayNode)

  if opts.run_data_quality:
    if opts.run_playground:
      for category in veto_categories:
        parentNodes.append(hipePlayVetoNode[category])
    if opts.run_full_data:
      for category in veto_categories:
        parentNodes.append(hipeAnalysisVetoNode[category])

  if opts.run_injections:
    for (injDir, injSeed) in cp.items("injections"):
      if opts.run_search:
        parentNodes.append(hipeInjNode[injDir])
      if opts.run_data_quality:
        for category in veto_categories:
          parentNodes.append(hipeInjVetoNode[category][injDir])

  if len(parentNodes) == 0:
    parentNodes = None

  playgroundOnly = False

  print "Running lalapps_pipedown"
  dag = inspiralutils.pipedownSetup(dag,cp,opts.log_path,"pipedown",\
                            "../" + cachefilename,parentNodes,playgroundOnly)

##############################################################################
# Set up the directories for plotting and run lalapps_plot
if opts.run_plots:
  if opts.run_playground:

    # set up parents correctly
    if opts.run_search: 
      parentNodes = [hipePlayNode]
    else: parentNodes = None

    if opts.run_data_quality:
      parentVetoNodes = [hipePlayVetoNode]
    else: parentVetoNodes = None
    
    # make the playground slide/zerolag plots
    print "Making plots depending only on the playground"
    dag = inspiralutils.zeroSlidePlots(dag, "playground_summary_plots", cp, \
        opts.log_path, "PLAYGROUND", "PLAYGROUND", "../" + cachefilename, \
        opts.do_dag_categories, parentNodes, parentVetoNodes, \
        veto_categories, ifos)

  if opts.run_playground and opts.run_full_data:

    # set up parents correctly
    if opts.run_search: 
      parentNodes = [hipePlayNode, hipeAnalysisNode]
    else: parentNodes = None

    if opts.run_data_quality and opts.run_playground:
      parentVetoNodes = [hipePlayVetoNode, hipeAnalysisVetoNode]
    else: parentVetoNodes = None

    # make the playground slide/zerolag plots
    print "Making plots with full data slides, playground zero lag"
    dag = inspiralutils.zeroSlidePlots(dag, "full_data_slide_summary_plots", \
        cp, opts.log_path, "PLAYGROUND", "FULL_DATA", "../" + cachefilename, \
        opts.do_dag_categories, parentNodes, parentVetoNodes, \
        veto_categories, ifos)
 
  if opts.run_full_data:

    # set up parents correctly
    parentNodes = None
    parentVetoNodes = None
    if opts.run_full_data:
      if opts.run_search:
        parentNodes = [hipeAnalysisNode]
      if opts.run_data_quality:
        parentVetoNodes = [hipeAnalysisVetoNode]

    # make the full data slide/zerolag plots
    print "Making full data zero lag plots."
    dag = inspiralutils.zeroSlidePlots(dag, "full_data_summary_plots", cp, \
          opts.log_path, "FULL_DATA", "FULL_DATA", "../" + cachefilename, \
          opts.do_dag_categories, parentNodes, parentVetoNodes, veto_categories , ifos)


  # Setting up plots for software injection run.
  if opts.run_injections:
    print "Making plots depending on the injection runs"
    if opts.run_full_data:
      slideSuffix = "FULL_DATA"
    elif opts.run_playground:
      slideSuffix = "PLAYGROUND"
    else:
      slideSuffix = "NONE_AVAILABLE"
    if opts.run_playground:
      zeroLagSuffix = "PLAYGROUND"
    else:
      zeroLagSuffix = "NONE_AVAILABLE"
    for (injDir, injSeed) in cp.items("injections"):
  
      # set up parents correctly
      if opts.run_search: 
        parentNodes=[hipeInjNode[injDir]]
        if opts.run_full_data:
          parentNodes.append(hipeAnalysisNode)
        if opts.run_playground:
          parentNodes.append(hipePlayNode)
      else: parentNodes = None

      if opts.run_data_quality:
        parentVetoNodes = [hipeInjVetoNode[veto_categories[-1]][injDir]]
        if opts.run_full_data:
          parentVetoNodes.append(hipeAnalysisVetoNode[veto_categories[-1]])
        if opts.run_playground:
          parentVetoNodes.append(hipePlayVetoNode[veto_categories[-1]])
      else: parentVetoNodes = None

      dag = inspiralutils.injZeroSlidePlots(dag, injDir + "_summary_plots", \
          cp, opts.log_path, injDir.upper(), zeroLagSuffix, slideSuffix, \
          "../" + cachefilename, opts.do_dag_categories, parentNodes, 
          parentVetoNodes, veto_categories, ifos)

    print "Making plots of all injection runs together"

    # set up parents correctly
    if opts.run_search: 
      parentNodes = hipeInjNode.values()
      if opts.run_full_data:
        parentNodes.append(hipeAnalysisNode)
      if opts.run_playground:
        parentNodes.append(hipePlayNode)
    else: parentNodes = None

    if opts.run_data_quality:
      parentVetoNodes = hipeInjVetoNode[veto_categories[-1]].values()
      if opts.run_playground:
        parentVetoNodes.append(hipePlayVetoNode[veto_categories[-1]])
      if opts.run_full_data:
        parentVetoNodes.append(hipeAnalysisVetoNode[veto_categories[-1]])
    else: parentVetoNodes = None

    dag = inspiralutils.injZeroSlidePlots(dag, "allinj_summary_plots", cp, \
        opts.log_path, "*INJ", zeroLagSuffix, slideSuffix, \
        "../" + cachefilename, opts.do_dag_categories, parentNodes, 
        parentVetoNodes, veto_categories, ifos)

  if opts.do_dag_categories:
    dag.add_maxjobs_category('plotting',2)

##############################################################################
# Set up jobs for HW injection page

if opts.run_hardware_inj:
  # Set up parents correctly
  parentNodes = []
  if opts.run_search:
    if opts.run_full_data:
      parentNodes.append(hipeAnalysisNode)

    if opts.run_full_data:
      for category in veto_categories:
        parentNodes.append(hipeAnalysisVetoNode[category])

  HWinjNodes = inspiralutils.hwinj_page_setup(cp,ifos,veto_categories,hw_inj_dir)
  for node in HWinjNodes:
    for parent in parentNodes:
      node.add_parent(parent)
    dag.add_node(node)

##############################################################################
# Set up the directories for followup and run lalapps_followup_pipe
if opts.run_followup:
  followupNode = inspiralutils.followup_setup("followup", cp, opts, "full_data")
  # FIXME: the cache files need to be appended to cachelist
  dag.add_node(followupNode)
  if opts.run_search:
    followupNode.add_parent(hipeAnalysisNode)

##############################################################################
# set the number of retries for each of the sub-dags run by ihope
try:
  num_retries = int(cp.get('pipeline','retry-subdag'))
  for node in dag.get_nodes():
    node.set_retry(num_retries)
except:
  # do not retry nodes
  pass

##############################################################################
# Write the files needed to invoke pegasus

ifo_frame_type_dict = {}
for ifo in ifos:
  ifo_frame_type_dict[ifo] = inspiralutils.get_data_options(cp,ifo)[1]

pfnfile = inspiralutils.create_frame_pfn_file(ifo_frame_type_dict,opts.gps_start_time,opts.gps_end_time)
peg_frame_cache = inspiralutils.create_pegasus_cache_file(pfnfile)

peg_fh = open("pegasus_submit_dax", "w")

#set up site and dir options
if opts.grid_site:
  exec_site=opts.grid_site
  dirs_entry='--relative-dir ' + os.path.basename(tmp_exec_dir) + ' --relative-submit-dir .'
else:
  exec_site='local'
  dirs_entry='--relative-dir .' 

print >> peg_fh, """#!/bin/bash

TMP_EXEC_DIR=%s
IHOPE_RUN_DIR=%s
UEBER_CONCRETE_DAG=%s

usage()
{
  echo "Usage: pegasus_submit_dag [-f] [-h]"
  echo
  echo "  -f, --force           Force re-plan and resubmit of DAX"
  echo "  -h, --help            Print this message"
  echo
}

if [ $# -gt 1 ] ; then
  usage
  exit 1
fi

if [ $# -eq 1 ] ; then
 if [ $1 = "-h" ] || [ $1 = "--help" ] ; then
   usage
   exit 0
 fi

 if [ $1 = "-f" ] || [ $1 = "--force" ] ; then 
   echo "WARNING: removing any existing workflow files!"
   pegasus-remove ${TMP_EXEC_DIR}/.
   echo "Sleeping for 60 seconds to give running DAGs chance to exit..."
   sleep 60
   rm -rf ${TMP_EXEC_DIR}
   mkdir ${TMP_EXEC_DIR}
   chmod 755 ${TMP_EXEC_DIR}
 else
   usage
   exit 1
 fi
fi

if [ -f ${TMP_EXEC_DIR}/${UEBER_CONCRETE_DAG}.lock ] ; then
  echo
  echo "ERROR: A dagman lock file already exists which may indicate that your"
  echo "workflow is already running. Please check the status of your DAX with"
  echo
  echo "    pegasus-status ${TMP_EXEC_DIR}/."
  echo
  echo "If necessary, you can remove the workflow with"
  echo
  echo "    pegasus-remove ${TMP_EXEC_DIR}/."
  echo
  echo "You can also run"
  echo
  echo "    pegasus_submit_dax -f"
  echo
  echo "to force the workflow to re-run. This will remove any existing"
  echo "workflow log and error files. If these need to be preserved,"
  echo "you must back them up first before running with -f."
  echo
  exit 1
fi

# The theory here is to find the longest living
# proxy certificate and copy it into the default
# location so that workflows can find it even after
# the user has logged out. This is necessary because
# Condor and condor_dagman do not yet properly cache
# and manage credentials to make them available to all
# jobs in the workflow, and other tools make assumptions
# about where a proxy, service, or user certificate is located 
# on the file system and do not properly find valid 
# existing credentials using the proper GSI search algorithm.
#
# This is not a great solution because there can be quite
# valid reasons to have multiple credentials with different
# lifetimes, and it presents a security risk to circumvent
# the state and move files around without knowing why the
# state is the way it is.
#
# But to support LIGO users we are doing it at this time
# until better tooling is available.
#
# Assumes grid-proxy-info is in PATH

if ! `/usr/bin/which grid-proxy-info > /dev/null 2>&1` ; then
    echo "ERROR: cannot find grid-proxy-info in PATH";
    exit 1
fi

# default location for proxy certificates based on uid
x509_default="/tmp/x509up_u`id -u`"

# if X509_USER_PROXY is defined and has a lifetime of > 1 hour
# compare to any existing default and copy it into place if 
# and only if its lifetime is greater than the default
if [ -n "$X509_USER_PROXY" ] ; then
    echo "X509_USER_PROXY=${X509_USER_PROXY}"
    if `grid-proxy-info -file ${X509_USER_PROXY} -exists -valid 1:0` ; then
        nsec=`grid-proxy-info -file ${X509_USER_PROXY} -timeleft`
        echo "Lifetime of ${X509_USER_PROXY} ${nsec} seconds"
        if [ -e ${x509_default} ] ; then
            echo "Proxy exists at default location"
            if `grid-proxy-info -file ${x509_default} -exists -valid 1:0` ; then
                nsec=`grid-proxy-info -file ${X509_USER_PROXY} -timeleft`
                echo "Lifetime of default ${nsec} seconds"
                env_life=`grid-proxy-info -file ${X509_USER_PROXY} -timeleft`
                def_life=`grid-proxy-info -file ${x509_default} -timeleft`
                if [ ${env_life} -gt ${def_life} ] ; then
                    cp ${X509_USER_PROXY} ${x509_default}
                    echo "Lifetime of ${X509_USER_PROXY} > default"
                    echo "Copied ${X509_USER_PROXY} into default location"
                else
                    echo "Lifetime of default > ${X509_USER_PROXY}"
                    echo "Leaving default in place"
                fi
            else
                echo "Lifetime of default < 1 hour"
                cp ${X509_USER_PROXY} ${x509_default}
                echo "Lifetime of ${X509_USER_PROXY} > default"
                echo "Copied ${X509_USER_PROXY} into default location"
            fi
        else
            echo "No proxy at default location"
            cp ${X509_USER_PROXY} $x509_default
            echo "Copied ${X509_USER_PROXY} into default location"
        fi
    else
        echo "Lifetime of ${X509_USER_PROXY} < 1 hour"
        echo "Ignoring ${X509_USER_PROXY}"
        echo "Assuming default location for proxy"
    fi
else
    echo "X509_USER_PROXY not set"
    echo "Assuming default location for proxy"
fi

# when we get here we can assume that if a valid proxy with lifetime > 1 exists
# then it is in the default location, so test for it now
valid=`grid-proxy-info -file ${x509_default} -exists -valid 1:0 > /dev/null 2>&1`
if ! ${valid} ; then
    echo "ERROR: could not find proxy with lifetime > 1 hour"
    exit 1
fi

# if we get here valid proxy with lifetime > 1 hour was 
# found so print out details for the record
grid-proxy-info -file ${x509_default} -all

# set X509_USER_PROXY to the default now
X509_USER_PROXY=${x509_default}
export X509_USER_PROXY

# set specific condor variables needed by pegasus
export _CONDOR_DAGMAN_LOG_ON_NFS_IS_ERROR=FALSE
export _CONDOR_DAGMAN_COPY_TO_SPOOL=False

if [ -f ${TMP_EXEC_DIR}/${UEBER_CONCRETE_DAG} ] ; then
  pegasus-run --conf ${IHOPE_RUN_DIR}/pegasus.properties ${TMP_EXEC_DIR}/.
else
  pegasus-plan --conf ${IHOPE_RUN_DIR}/pegasus.properties \\
               --dax %s \\
               --dir ${TMP_EXEC_DIR} \\
               %s -s %s --nocleanup -f --submit

  ln -sf ${TMP_EXEC_DIR}/${UEBER_CONCRETE_DAG}.dagman.out ${UEBER_CONCRETE_DAG}.dagman.out
fi
""" % ( tmp_exec_dir, os.getcwd(), 
        dag.get_dax_file().replace('.dax','') + '-0.dag', 
        dag.get_dax_file(), dirs_entry, exec_site )
peg_fh.close()
os.chmod("pegasus_submit_dax",0755)

###############################################################################
# write a shell script that can return the basedir and uber-concrete-dag

basedir_fh = open("pegasus_basedir", "w")

print >> basedir_fh, """#!/bin/bash

TMP_EXEC_DIR=%s
UEBER_CONCRETE_DAG=%s

usage()
{
  echo "Usage: pegasus_basedir [-d]"
  echo
  echo "Prints the name of the Pegasus basedir where the condor files can be found"
  echo
  echo "  -d, --dag             Append the name of the concrete DAG to the basedir"
  echo "  -h, --help            Print this message"
  echo
}

if [ $# -gt 1 ] ; then
  usage
  exit 1
fi


if [ $# -eq 1 ] ; then
 if [ $1 = "-h" ] || [ $1 = "--help" ] ; then
   usage
   exit 0
 fi

 if [ $1 = "-d" ] || [ $1 = "--dag" ] ; then
   echo ${TMP_EXEC_DIR}/${UEBER_CONCRETE_DAG}
   exit 0
 else
   usage
   exit 1
 fi
fi

echo ${TMP_EXEC_DIR}
exit 0
""" % ( tmp_exec_dir, dag.get_dax_file().replace('.dax','') + '-0.dag' )
basedir_fh.close()
os.chmod("pegasus_basedir",0755)

pegprop_fh = open("pegasus.properties", "w")
print >> pegprop_fh, """
###############################################################################
# pegasus properties file generated by ihope
pegasus.catalog.replica=File
pegasus.catalog.replica.file=%s

pegasus.transfer.force=true
pegasus.transfer.links=true

dagman.retry=3
dagman.maxpre=1
pegasus.condor.arguments.quote=true

pegasus.selector.site=RoundRobin
pegasus.selector.replica=Default

pegasus.clusterer.job.aggregator.seqexec.firstjobfail=true

pegasus.dir.submit.subwf.labelbased=true
pegasus.dir.submit.logs=%s

pegasus.catalog.site.file=%s/site-local.xml
""" % (os.path.join(os.getcwd(),peg_frame_cache),opts.log_path,os.getcwd())

if opts.data_checkpoint:
  print >> pegprop_fh, """\

# pegasus 4.x variable for data checkpointing
pegasus.data.configuration=condorio
"""

pegprop_fh.close()

##############################################################################
# Write the dag and sub files
dag.write_sub_files()
dag.write_dag()

print 
print "Created a workflow file which can be submitted by executing"
print "\n    cd " + analysisDirectory
print """
and then

    ./pegasus_submit_dax

from a condor submit machine. 

From the analysis directory on the condor submit machine, you can run the
command

    pegasus-status --long -t -i `./pegasus_basedir`

to check the status of your workflow. Once the workflow has finished you 
can run the command

    pegasus-analyzer -t -i `./pegasus_basedir`

to debug any failed jobs.
"""

##############################################################################
# write out a log file for this script
log_fh = open(basename + '.pipeline.log', 'w')
  
# FIXME: the following code uses obsolete CVS ID tags.
# It should be modified to use git version information.
log_fh.write( "$Id$" \
    + "\n" )
log_fh.write( "$Name$" + "\n\n" )
log_fh.write( "Invoked with arguments:" )
for arg in command_line:
  if arg[0] == '-':
    log_fh.write( "\n" )
  log_fh.write( arg + ' ')

log_fh.write( "\n" )
log_fh.write( "Config file has CVS strings:\n" )
log_fh.write( cp.get('pipeline','version') + "\n" )
log_fh.write( cp.get('pipeline','cvs-tag') + "\n\n" )

##############################################################################
