#!/usr/bin/env @PYTHONPROG@

usage = \
"""
Program to construct post-processing dag.
"""

__author__ = 'Collin Capano <cdcapano@physics.syr.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import os, sys, re
import ConfigParser
from optparse import OptionParser
import tempfile
sys.path.append('@PYTHONLIBDIR@')

##############################################################################
# import the modules we need to build the pipeline
from glue import pipeline
from glue import lal
from glue.ligolw import lsctables
import inspiral

##############################################################################
# Function Definitions

def get_veto_segments_name( veto_cat_num, cumulative = True ):
  """
  Given a category number, returns a veto segments name 
  as set by segs_from_cats.

  @veto_cat_num: integer representing the category veto
  @cumulative: If set to True, will add CUMULATIVE to the name.
  """
  if cumulative:
    return ''.join([ 'VETO_CAT', str(veto_cat_num), '_CUMULATIVE' ])
  else:
    return ''.join([ 'VETO_CAT', str(veto_cat_num) ])
    

##############################################################################
#
#  MAIN PROGRAM
#
##############################################################################

##############################################################################
# parse command-line arguments
parser = OptionParser( version = "", usage = usage )

parser.add_option( "", "--ihope-cache", action = "store", type = "string",
  default = None,
  help =
    "The ihope cache to read."
  )
parser.add_option( "", "--config-file", action = "store", type = "string",
  default = None,
  help =
    "The .ini file to use."
  )
parser.add_option( "", "--log-path", action = "store", type = "string",
  default = None,
  help =
    "Directory to write condor log file and perform SQLite operation in. " +
    "Should be a local directory."
  )
parser.add_option( "", "--gps-start-time", action = "store", type = "string",
  default = None,
  help =
    "GPS start time of the ihope run."
  )
parser.add_option( "", "--gps-end-time", action = "store", type = "string",
  default = None,
  help =
    "GPS end time of the ihope run."
  )
parser.add_option( "", "--plot-playground-only", action = "store_true",
  default = False,
  help =
    "Turn on if want plots of playground data only."
  )

(options, args) = parser.parse_args()

##############################################################################
# Sanity check of input arguments
if not options.ihope_cache:
  raise ValueError, "An ihope-cache file is required."
if not options.config_file:
  raise ValueError, "A config-file is required."
if not options.log_path:
  raise ValueError, "A log-path is required."

##############################################################################
# parse the ini file and initialize
cp = ConfigParser.ConfigParser()
cp.read(options.config_file)

experiment_start = options.gps_start_time
experiment_duration = str(int(options.gps_end_time) - int(options.gps_start_time))

# if logs directory not present, create it
try:
  os.mkdir('logs/')
except:
  pass

##############################################################################
# create a log file that the Condor jobs will write to
basename = re.sub(r'\.ini', r'', options.config_file)

tempfile.tempdir = options.log_path
tempfile.template = '.'.join([ basename, 'dag.log.' ])
logfile = tempfile.mktemp()
fh = open( logfile, "w" )
fh.close()

##############################################################################
# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile)
dag.set_dag_file(basename)

##############################################################################
# Open the ihope cache and sort THINCA_SECOND files by user_tag

ihope_cache = [line for line in file(options.ihope_cache) \
  if "THINCA_SECOND" in line or "THINCA_SLIDE_SECOND" in line or " INJECTIONS" in line]

zero_lag_cache = lal.Cache([lal.CacheEntry(entry) for entry in ihope_cache \
  if "THINCA_SECOND" in entry])
slide_cache = lal.Cache([lal.CacheEntry(entry) for entry in ihope_cache \
  if "THINCA_SLIDE_SECOND" in entry])
inj_cache = lal.Cache([lal.CacheEntry(entry) for entry in ihope_cache if \
  " INJECTIONS" in entry])

del ihope_cache

# get the user tags from the zero_lag_cache
user_tags = set([ '_'.join([ entry.description.split('_')[ii] 
  for ii in range( 3, len(entry.description.split('_')) ) ])
  for entry in zero_lag_cache ])

##############################################################################
# Cycle over the user tags, creating a pipeline for each

for tag in user_tags:
  
  print "Creating jobs for %s..." % tag
  
  # determine whether or not this was an injection run by checking if there
  # is an injection file

  file_sieve = ''.join([ 'INJECTION*', tag.split('_CAT')[0], '*' ])
  inj_cache = ihope_cache.sieve( description = file_sieve, exact_match = True )
  if len(inj_cache) == 0:
    simulation = False
  if len(inj_cache) == 1:
    simulation = True
  if len(inj_cache) > 1:
    raise ValueError, "More than one injection file found for %s" % tag

  # write the jobs that will be run
  t2c_job = inspiral.ThincaToCoincJob(cp)
  tosql_job = pipeline.LigolwSqliteJob(cp)
  dbsimplify_job = inspiral.DBSimplifyJob(cp)
  cluster_job = inspiral.ClusterCoincsJob(cp)
  ucfar_job = inspiral.CFarJob(cp, ['cfar-uncombined'])
  ccfar_job = inspiral.CFarJob(cp, ['cfar-combined'])
  if simulation:
    dbaddinj_job = inspiral.DBAddInjJob(cp)
  else:
    plotslides_job = inspiral.PlotSlidesJob(cp)
    plotifar_job = inspiral.PlotIfarJob(cp)

  # set better submit file names than the default
  subsuffix = 'sub'
  t2c_job.set_sub_file( '.'.join([ basename, tag, 'thinca_to_coinc', 'sub' ]) )
  tosql_job.set_sub_file( '.'.join([ basename, tag, 'ligolw_sqlite', 'sub' ]) )
  dbsimplify_job.set_sub_file( '.'.join([ basename, tag, 'dbsimplify', 'sub' ]) )
  cluster_job.set_sub_file( '.'.join([ basename, tag, 'cluster_coincs', 'sub' ]) )
  ucfar_job.set_sub_file( '.'.join([ basename, tag, 'uncombined.cfar', 'sub' ]) )
  ccfar_job.set_sub_file( '.'.join([ basename, tag, 'combined.cfar', 'sub' ]) )
  if simulation:
    dbaddinj_job.set_sub_file ( '.'.join([ basename, tag, 'dbaddinj', 'sub' ]) )
  else:
    plotslides_job.set_sub_file( '.'.join([ basename, tag, 'plotslides', 'sub' ]) )
    plotifar_job.set_sub_file( '.'.join([ basename, tag, 'plotifar', 'sub' ]) )

  ############################################################################
  # Step 1: Setup thinca_to_coinc nodes

  print "\tsetting up thinca_to_coinc nodes..."

  # set job options
  t2c_job.set_experiment_start_time(options.gps_start_time)
  t2c_job.set_experiment_end_time(options.gps_end_time)
  if simulation:
    t2c_job.set_simulation()

  # sieve the ihope_cache for THINCA_SECOND files with this tag
  file_sieve = 'THINCA_SECOND*' + tag
  thinca_cache = ihope_cache.sieve( description = file_sieve, exact_match = True )

  # if not an injection run, check that there are an equal number of
  # slide files
  file_sieve = 'THINCA_SLIDE_SECOND*' + tag
  if not simulation and \
    len(ihope_cache.sieve( description = file_sieve, exact_match = True )) != \
    len(thinca_cache):
    raise ValueError, "Number of %s slide files doesn't equal number of zero-lag files. " % tag  + \
      "Have all the jobs completed?"

  # get distinct on_instruments in the thinca cache
  distinct_instrument_sets = set([ entry.observatory for entry in thinca_cache ])
  # from the distinct_instrument_sets figure out what distinct ifos there are
  distinct_ifos = set()
  for on_instruments in distinct_instrument_sets:
    distinct_ifos |= lsctables.instrument_set_from_ifos(on_instruments)
  distinct_ifos = ''.join(sorted(distinct_ifos))

  # get the veto file
  if 'VETO' in tag:
    # get the category veto (assuming tag has form: *CAT_N_VETO)
    cat_num = tag.split('_')[-2]
  else:
    # is category 1 veto
    cat_num = '1'
  veto_file_path = cp.get('input', 'ihope-segments-directory')
  veto_file_name = ''.join([
    distinct_ifos, '-VETOTIME_CAT_', cat_num, '-', experiment_start, '-', experiment_duration, 
    '.xml' ])
  veto_file = '/'.join([ veto_file_path, veto_file_name ])
  if not os.path.exists( veto_file ):
    raise ValueError, "Veto file %s could not be found." % veto_file
  # get the veto segments name
  veto_segs_name = get_veto_segments_name( cat_num, cumulative = True )

  t2c_output = []
  cache_start_time = int(experiment_start)
  for on_instruments in distinct_instrument_sets:
    instrument_cache = thinca_cache.sieve( ifos = on_instruments, exact_match = True )
    instrument_cache.sort()
    this_thinca_url_list = []
    for entry_num, entry in enumerate(instrument_cache):
      if not this_thinca_url_list:
        # first entry; store type for cache naming
        cache_type = entry.description
      # Add the zero-lag thinca url to the url list
      this_thinca_url_list.append(entry.url)
      # if this isn't a simulation add the slide file
      if not simulation:
        slide_url = re.sub('THINCA', 'THINCA_SLIDE', entry.url)
        this_thinca_url_list.append(slide_url)
      # if n*20th entry or last remainder, write to cache
      if (entry_num + 1) % 20 == 0 or (entry_num + 1) == len(instrument_cache):
        # set duration to be end of this entry - start of first entry in cache
        cache_duration = entry.segment[1].seconds - cache_start_time
        this_cache = lal.Cache().from_urls( this_thinca_url_list )
        this_cache_name = '.'.join([ 
          '-'.join([on_instruments, cache_type, str(cache_start_time), str(cache_duration)]),
          'cache' ])
        this_cache_file = open( this_cache_name, 'w' )
        this_cache.tofile(this_cache_file)
        this_cache_file.close()
        # write node
        this_t2c_node = inspiral.ThincaToCoincNode(t2c_job)
        input_instruments = lsctables.ifos_from_instrument_set(
          lsctables.instrument_set_from_ifos( on_instruments ) )
        this_t2c_node.set_instruments(input_instruments)
        this_t2c_node.set_input_cache(this_cache_name)
        this_t2c_node.set_veto_segments( veto_file )
        this_t2c_node.set_veto_segments_name( veto_segs_name )
        dag.add_node(this_t2c_node)
        # add output files to t2c_output
        t2c_output += this_t2c_node.get_output_from_cache()
        # set end-time of last entry to be start time of next cache
        cache_start_time = entry.segment[1].seconds
        this_thinca_url_list = []
      
  # write output cache
  output_cache = lal.Cache().from_urls(t2c_output)
  # create cache name from what's in the output cache
  cache_type = output_cache[0].description
  t2c_output_cache = '.'.join([ 
    '-'.join([ distinct_ifos, cache_type, experiment_start, experiment_duration ]), 
    'cache' ])
  fp = open( t2c_output_cache, 'w' )
  output_cache.tofile( fp )
  fp.close()

  ############################################################################
  # Step 2a: Setup a LigolwSqliteNode for putting thinca_to_coincs 
  # into a sql db

  print "\tsetting up node to put thinca_to_coinc files into a SQLite database..."
  
  # set node options
  t2c2sql_node = pipeline.LigolwSqliteNode( tosql_job )
  t2c2sql_node.set_input_cache( t2c_output_cache )
  t2c2sql_node.set_tmp_space( options.log_path )
  # database name has form: 
  # distinct_ifos-CBC_TRIGDB_RAW-USER_TAG-cache_start-cache_duration.sql
  db_type = 'CBC_TRIGDB_RAW-' + tag
  raw_result_db = '.'.join([
    '-'.join([ distinct_ifos, db_type, experiment_start, experiment_duration ]),
    'sql' ])
  # check to make sure the database doesn't already exist
  if os.path.exists( raw_result_db ):
    raise ValueError, "Raw result database %s already exists. " % raw_result_db + \
    "Either rm or mv it, then re-run. (If it exists, you may also want to " + \
    "mv the CLUSTERED database so it doesn't get overwritten by cluster_coincs.)" \
    
  
  t2c2sql_node.set_database( raw_result_db )
  
  # set parent nodes to be all the thinca_to_coinc nodes
  [t2c2sql_node.add_parent( node ) for node in dag.get_nodes() \
    if isinstance(node, inspiral.ThincaToCoincNode)]
      
  dag.add_node( t2c2sql_node )
  
  ############################################################################
  # Step 2b: Setup a LigolwSqliteNode for putting the veto-segments file 
  # into the raw database
  
  # cache the veto file
  veto_cache = lal.Cache().from_urls( [veto_file] )
  veto_cache_file = re.sub('.xml', '.cache', os.path.basename(veto_file))
  fp = open( veto_cache_file, 'w' )
  veto_cache.tofile( fp )
  fp.close()
  
  # write node to add the veto file
  veto2sql_node = pipeline.LigolwSqliteNode( tosql_job )
  veto2sql_node.set_input_cache( veto_cache_file )
  veto2sql_node.set_tmp_space( options.log_path )
  veto2sql_node.set_database( raw_result_db )
  
  # set parent node
  veto2sql_node.add_parent( t2c2sql_node )
  
  dag.add_node( veto2sql_node )
  
  ############################################################################
  # Step 3: Setup a DBSimplifyNode to clean up the output of the t2c2sql_node 
  
  print "\tsetting up dbsimplify node to clean the database..."
  
  # set node options
  dbsimplify_node = inspiral.DBSimplifyNode( dbsimplify_job )
  dbsimplify_node.set_tmp_space( options.log_path )
  dbsimplify_node.set_database( raw_result_db )
  
  # set parent node
  dbsimplify_node.add_parent( veto2sql_node )
  
  dag.add_node( dbsimplify_node )
  
  
  ############################################################################
  # Step 3c: If simulation, add injection file.
  
  if simulation:
    print "\tsetting up dbaddinj node to add the injection file..."
  
    # set node options
    dbaddinj_node = inspiral.DBAddInjNode( dbaddinj_job )
    dbaddinj_node.set_tmp_space( options.log_path )
    dbaddinj_node.set_database( raw_result_db )
    dbaddinj_node.set_injection_file( inj_cache[0].url )
  
    dbaddinj_node.add_parent( dbsimplify_node )
  
    dag.add_node( dbaddinj_node )
  
  ############################################################################
  # Step 4: Setup a ClusterCoincsNode to cluster the output of dbsimplify_node
  
  print "\tsetting up cluster node to cluster coincs in the database..."
  
  # set node options
  cluster_node = inspiral.ClusterCoincsNode( cluster_job )
  cluster_node.set_tmp_space( options.log_path )
  cluster_node.set_input( raw_result_db )
  # output database name has form:
  # distinct_ifos-CBC_TRIGDB_CLUSTERED-USER_TAG-gps_start_time-durations.sql
  db_type = 'CBC_TRIGDB_CLUSTERED-' + tag
  result_db = '.'.join([
    '-'.join([ distinct_ifos, db_type, experiment_start, experiment_duration ]),
    'sql' ]) 
  cluster_node.set_output(result_db)
  
  # set parent node
  cluster_node.add_parent( dbsimplify_node )
  if simulation:
    cluster_node.add_parent( dbaddinj_node )
  
  dag.add_node( cluster_node )
  
  ############################################################################
  # Step 5a: Setup a CfarNode to compute the uncombined false alarm rates
  
  print "\tsetting up cfar nodes:"
  print "\t\tfor uncombined false alarm rates..."
  
  # set node options: output database is same as input
  ucfar_node = inspiral.CFarNode( ucfar_job )
  ucfar_node.set_tmp_space( options.log_path )
  ucfar_node.set_input( result_db )
  ucfar_node.set_output( result_db )
  
  # set parent node
  ucfar_node.add_parent( cluster_node )
  
  dag.add_node( ucfar_node )
  
  ############################################################################
  # Step 5b: Setup a CfarNode to compute the combined false alarm rates
  
  print "\t\tfor combined false alarm rates..."
  
  # set node options: output database is same as input
  ccfar_node = inspiral.CFarNode( ccfar_job )
  ccfar_node.set_tmp_space( options.log_path )
  ccfar_node.set_input( result_db )
  ccfar_node.set_output( result_db )
  
  # set parent node
  ccfar_node.add_parent( ucfar_node )
  
  dag.add_node( ccfar_node )

  ############################################################################
  # Plotting: Generate all result plots
  
  print "\tsetting up plotting jobs..."
  
  if not simulation:
    print "\t\tcreating plotslides node..."
    # set plotslides_job options
    if options.plot_playground_only:
      plotslides_job.set_plot_playground_only()
    # Write plotslides node
    plotslides_node = inspiral.PlotSlidesNode( plotslides_job )
    plotslides_node.set_tmp_space( options.log_path )
    plotslides_node.set_input( result_db )
    plotslides_node.set_user_tag( tag )
  
    # set parent node
    plotslides_node.add_parent( ccfar_node )
  
    dag.add_node( plotslides_node )
  
    # Write plotifar nodes for different datatypes
    print "\t\tcreating plotifar node for datatypes:"
    for datatype in ['all_data', 'playground', 'exclude_play']: 
      # only create nodes for non-playground if options.plot-playground-only not set
      if options.plot_playground_only  and datatype != 'playground':
        continue
      print "\t\t\t%s..." % datatype
      plotifar_node = inspiral.PlotIfarNode( plotifar_job )
      plotifar_node.set_tmp_space( options.log_path )
      plotifar_node.set_input( result_db )
      plotifar_node.set_datatype( datatype )
      plotifar_node.set_user_tag( tag )
      
      # set parent node
      plotifar_node.add_parent( ccfar_node )
  
      dag.add_node( plotifar_node )
  
  
##############################################################################
##############################################################################
# Final Step: Write the DAG

print "Writing DAG and sub files..."

dag.write_sub_files()
dag.write_dag()

print "Finished!"
print "Now run:\n\tcondor_submit_dag %s" % os.path.basename(dag.get_dag_file())
sys.exit(0)

